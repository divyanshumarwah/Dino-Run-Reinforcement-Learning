{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\nic\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\nic\\anaconda3\\lib\\site-packages (from selenium) (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n",
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2  #opencv\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import io\n",
    "from io import BytesIO\n",
    "import time\n",
    "import os\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"chrome://dino\"\n",
    "chromedriver_path = \"D:/TCD/Sem 2/AI/DinoRunTutorial-master/chromedriver.exe\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chromedriver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.get('chrome://dino')\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dino_Agent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "    def running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_state:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1\n",
    "        game_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            game_over = True\n",
    "        return image, reward, game_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, name):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_object(name):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = img_process(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def img_process(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[:300, :500] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH_SIZE = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_object(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_object(t,\"time\")\n",
    "    D = deque()\n",
    "    save_object(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def train_Network(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_object(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_object(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_object(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH_SIZE)\n",
    "            inputs = np.zeros((BATCH_SIZE, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = targets[i, action_t]+reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = (targets[i, action_t]* (1-LEARNING_RATE)) + (LEARNING_RATE * (reward_t + GAMMA * np.max(Q_sa)))\n",
    "                    # Q[s,a] = Q[s,a] * (1-alpha) + alpha* (reward + Gamma* np.max(Q[S',:]))\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_object(D,\"D\") #saving episodes\n",
    "            save_object(t,\"time\") #caching time steps\n",
    "            save_object(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \"/ Q_MAX \", np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "def play_Game(observe=False):\n",
    "    game = Game()\n",
    "    dino = Dino_Agent(game)\n",
    "    game_state = Game_state(dino,game)    \n",
    "    model = build_model()\n",
    "    try:\n",
    "        train_Network(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 1000 / STATE explore / EPSILON 0.09910189899999863 / ACTION 1 / REWARD 0.1 / Q_MAX  10.001173 / Loss  1.1436895874794573e-10\n",
      "fps: 1.2744212954522804\n",
      "TIMESTEP 1001 / STATE explore / EPSILON 0.09910089999999863 / ACTION 0 / REWARD 0.1 / Q_MAX  10.00116 / Loss  6.639311322942376e-11\n",
      "fps: 7.941486430963872\n",
      "TIMESTEP 1002 / STATE explore / EPSILON 0.09909990099999863 / ACTION 0 / REWARD 0.1 / Q_MAX  10.001167 / Loss  1.8144419300369918e-10\n",
      "fps: 7.466362859584164\n",
      "TIMESTEP 1003 / STATE explore / EPSILON 0.09909890199999863 / ACTION 0 / REWARD 0.1 / Q_MAX  10.001219 / Loss  7.787548383930698e-11\n",
      "fps: 11.635169285823267\n",
      "TIMESTEP 1004 / STATE explore / EPSILON 0.09909790299999863 / ACTION 0 / REWARD 0.1 / Q_MAX  10.0012665 / Loss  2.2509993868879974e-10\n",
      "fps: 11.370590962201083\n",
      "TIMESTEP 1005 / STATE explore / EPSILON 0.09909690399999863 / ACTION 0 / REWARD 0.1 / Q_MAX  10.001302 / Loss  4.8203219193965197e-11\n",
      "fps: 8.887800185202059\n",
      "TIMESTEP 1006 / STATE explore / EPSILON 0.09909590499999862 / ACTION 0 / REWARD 0.1 / Q_MAX  10.001334 / Loss  6.272671271290164e-11\n",
      "fps: 11.911947720734204\n",
      "TIMESTEP 1007 / STATE explore / EPSILON 0.09909490599999862 / ACTION 0 / REWARD 0.1 / Q_MAX  10.001387 / Loss  1.0231815394945443e-10\n",
      "----------Random Action----------\n",
      "fps: 7.096576835095223\n",
      "TIMESTEP 1008 / STATE explore / EPSILON 0.09909390699999862 / ACTION 1 / REWARD 0.1 / Q_MAX  10.001426 / Loss  7.440803528879769e-11\n",
      "fps: 9.62115491368197\n",
      "TIMESTEP 1009 / STATE explore / EPSILON 0.09909290799999862 / ACTION 0 / REWARD 0.1 / Q_MAX  10.001432 / Loss  7.344169716816396e-11\n",
      "fps: 8.269526813880127\n",
      "TIMESTEP 1010 / STATE explore / EPSILON 0.09909190899999862 / ACTION 0 / REWARD 0.1 / Q_MAX  10.0014105 / Loss  7.466383067367133e-11\n",
      "fps: 10.006116791594899\n",
      "TIMESTEP 1011 / STATE explore / EPSILON 0.09909090999999862 / ACTION 0 / REWARD 0.1 / Q_MAX  10.001385 / Loss  0.0017493355553597212\n",
      "fps: 8.479919776149579\n",
      "TIMESTEP 1012 / STATE explore / EPSILON 0.09908991099999861 / ACTION 0 / REWARD 0.1 / Q_MAX  9.989097 / Loss  6.488192593678832e-05\n",
      "fps: 9.351577323288288\n",
      "TIMESTEP 1013 / STATE explore / EPSILON 0.09908891199999861 / ACTION 0 / REWARD 0.1 / Q_MAX  9.9937315 / Loss  0.00011891917529283091\n",
      "fps: 10.370671473324778\n",
      "TIMESTEP 1014 / STATE explore / EPSILON 0.09908791299999861 / ACTION 0 / REWARD 0.1 / Q_MAX  10.020049 / Loss  2.1036204998381436e-05\n",
      "fps: 10.75939839980299\n",
      "TIMESTEP 1015 / STATE explore / EPSILON 0.09908691399999861 / ACTION 0 / REWARD 0.1 / Q_MAX  10.054669 / Loss  2.347487452425412e-06\n",
      "fps: 8.069600551017189\n",
      "TIMESTEP 1016 / STATE explore / EPSILON 0.09908591499999861 / ACTION 0 / REWARD 0.1 / Q_MAX  10.096691 / Loss  1.3592749382951297e-05\n",
      "fps: 10.210112488102453\n",
      "TIMESTEP 1017 / STATE explore / EPSILON 0.09908491599999861 / ACTION 0 / REWARD 0.1 / Q_MAX  10.138635 / Loss  1.345367195426661e-06\n",
      "fps: 8.701135380600405\n",
      "TIMESTEP 1018 / STATE explore / EPSILON 0.0990839169999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.183702 / Loss  8.3792838267982e-06\n",
      "fps: 10.210336181503932\n",
      "TIMESTEP 1019 / STATE explore / EPSILON 0.0990829179999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.230023 / Loss  1.3108772691339254e-05\n",
      "fps: 10.1072193666698\n",
      "TIMESTEP 1020 / STATE explore / EPSILON 0.0990819189999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.260464 / Loss  2.7870351004821714e-06\n",
      "fps: 9.621221122944972\n",
      "TIMESTEP 1021 / STATE explore / EPSILON 0.0990809199999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.2643 / Loss  7.799321610946208e-05\n",
      "fps: 8.855094371490098\n",
      "TIMESTEP 1022 / STATE explore / EPSILON 0.0990799209999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.321741 / Loss  2.269226797579904e-06\n",
      "fps: 5.003052428046076\n",
      "TIMESTEP 1023 / STATE explore / EPSILON 0.0990789219999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.376651 / Loss  4.7736248234286904e-05\n",
      "fps: 9.014696716343488\n",
      "TIMESTEP 1024 / STATE explore / EPSILON 0.0990779229999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.4345875 / Loss  6.099556776462123e-05\n",
      "fps: 9.907063644845465\n",
      "TIMESTEP 1025 / STATE explore / EPSILON 0.0990769239999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.48535 / Loss  6.1969385569682345e-06\n",
      "fps: 10.873030151418135\n",
      "TIMESTEP 1026 / STATE explore / EPSILON 0.0990759249999986 / ACTION 0 / REWARD 0.1 / Q_MAX  10.527694 / Loss  0.00010083732195198536\n",
      "----------Random Action----------\n",
      "fps: 10.532868253262718\n",
      "TIMESTEP 1027 / STATE explore / EPSILON 0.09907492599999859 / ACTION 0 / REWARD 0.1 / Q_MAX  10.583339 / Loss  1.2944376067025587e-05\n",
      "----------Random Action----------\n",
      "fps: 7.276371029859757\n",
      "TIMESTEP 1028 / STATE explore / EPSILON 0.09907392699999859 / ACTION 1 / REWARD 0.1 / Q_MAX  10.630264 / Loss  1.4747033674211707e-05\n",
      "fps: 11.24276336422144\n",
      "TIMESTEP 1029 / STATE explore / EPSILON 0.09907292799999859 / ACTION 0 / REWARD 0.1 / Q_MAX  10.2720785 / Loss  1.2953017176187132e-05\n",
      "fps: 11.078603785047346\n",
      "TIMESTEP 1030 / STATE explore / EPSILON 0.09907192899999859 / ACTION 0 / REWARD 0.1 / Q_MAX  10.693338 / Loss  1.2639688975468744e-05\n",
      "fps: 11.242914046458765\n",
      "TIMESTEP 1031 / STATE explore / EPSILON 0.09907092999999859 / ACTION 0 / REWARD 0.1 / Q_MAX  10.7106 / Loss  4.4196909584570676e-05\n",
      "fps: 8.338477075820169\n",
      "TIMESTEP 1032 / STATE explore / EPSILON 0.09906993099999858 / ACTION 0 / REWARD 0.1 / Q_MAX  10.726356 / Loss  1.950427940755617e-05\n",
      "fps: 9.907040244138642\n",
      "TIMESTEP 1033 / STATE explore / EPSILON 0.09906893199999858 / ACTION 0 / REWARD 0.1 / Q_MAX  10.72988 / Loss  1.2147245797677897e-05\n",
      "fps: 11.238395335626937\n",
      "TIMESTEP 1034 / STATE explore / EPSILON 0.09906793299999858 / ACTION 0 / REWARD 0.1 / Q_MAX  10.713057 / Loss  1.5677582268835977e-05\n",
      "fps: 12.027436856231791\n",
      "TIMESTEP 1035 / STATE explore / EPSILON 0.09906693399999858 / ACTION 0 / REWARD 0.1 / Q_MAX  10.192413 / Loss  1.7801192370825447e-05\n",
      "fps: 8.988038246752412\n",
      "TIMESTEP 1036 / STATE explore / EPSILON 0.09906593499999858 / ACTION 0 / REWARD 0.1 / Q_MAX  10.640754 / Loss  1.040157258103136e-05\n",
      "fps: 8.135086174459154\n",
      "TIMESTEP 1037 / STATE explore / EPSILON 0.09906493599999858 / ACTION 0 / REWARD 0.1 / Q_MAX  10.591677 / Loss  1.0718336852733046e-05\n",
      "fps: 9.014638591634661\n",
      "TIMESTEP 1038 / STATE explore / EPSILON 0.09906393699999858 / ACTION 0 / REWARD 0.1 / Q_MAX  10.534572 / Loss  9.834235243033618e-06\n",
      "fps: 9.351493923307254\n",
      "TIMESTEP 1039 / STATE explore / EPSILON 0.09906293799999857 / ACTION 0 / REWARD 0.1 / Q_MAX  10.470496 / Loss  7.65433105698321e-06\n",
      "fps: 9.81002121374426\n",
      "TIMESTEP 1040 / STATE explore / EPSILON 0.09906193899999857 / ACTION 0 / REWARD 0.1 / Q_MAX  10.4025545 / Loss  5.150047854840523e-06\n",
      "fps: 9.17993692260215\n",
      "TIMESTEP 1041 / STATE explore / EPSILON 0.09906093999999857 / ACTION 0 / REWARD 0.1 / Q_MAX  10.333376 / Loss  5.225928362051491e-06\n",
      "fps: 8.408536230867155\n",
      "TIMESTEP 1042 / STATE explore / EPSILON 0.09905994099999857 / ACTION 0 / REWARD 0.1 / Q_MAX  10.263049 / Loss  1.980626166186994e-06\n",
      "fps: 9.264941783537697\n",
      "TIMESTEP 1043 / STATE explore / EPSILON 0.09905894199999857 / ACTION 0 / REWARD 0.1 / Q_MAX  10.197967 / Loss  1.2931047876918456e-06\n",
      "fps: 9.922438367766004\n",
      "TIMESTEP 1044 / STATE explore / EPSILON 0.09905794299999857 / ACTION 0 / REWARD 0.1 / Q_MAX  10.141742 / Loss  1.1137964293084224e-06\n",
      "fps: 9.264798526221853\n",
      "TIMESTEP 1045 / STATE explore / EPSILON 0.09905694399999856 / ACTION 0 / REWARD 0.1 / Q_MAX  10.089269 / Loss  9.022620020004979e-07\n",
      "fps: 8.684362654563826\n",
      "TIMESTEP 1046 / STATE explore / EPSILON 0.09905594499999856 / ACTION 0 / REWARD 0.1 / Q_MAX  10.041402 / Loss  2.4271048459922895e-07\n",
      "fps: 6.138745497628251\n",
      "TIMESTEP 1047 / STATE explore / EPSILON 0.09905494599999856 / ACTION 0 / REWARD 0.1 / Q_MAX  9.999173 / Loss  2.615544417494675e-06\n",
      "fps: 5.211403125124249\n",
      "TIMESTEP 1048 / STATE explore / EPSILON 0.09905394699999856 / ACTION 0 / REWARD 0.1 / Q_MAX  9.9659815 / Loss  7.032472240098286e-07\n",
      "fps: 6.853639486128677\n",
      "TIMESTEP 1049 / STATE explore / EPSILON 0.09905294799999856 / ACTION 0 / REWARD 0.1 / Q_MAX  9.941022 / Loss  5.791549938294338e-06\n",
      "fps: 8.477708718632831\n",
      "TIMESTEP 1050 / STATE explore / EPSILON 0.09905194899999856 / ACTION 0 / REWARD 0.1 / Q_MAX  9.924536 / Loss  1.3227841918705963e-05\n",
      "fps: 11.501137965263268\n",
      "TIMESTEP 1051 / STATE explore / EPSILON 0.09905094999999856 / ACTION 0 / REWARD 0.1 / Q_MAX  9.918249 / Loss  1.5236517356242985e-05\n",
      "fps: 8.005038571793923\n",
      "TIMESTEP 1052 / STATE explore / EPSILON 0.09904995099999855 / ACTION 0 / REWARD 0.1 / Q_MAX  9.921971 / Loss  7.775661288178526e-06\n",
      "fps: 7.580428225198669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 1053 / STATE explore / EPSILON 0.09904895199999855 / ACTION 0 / REWARD 0.1 / Q_MAX  9.932766 / Loss  1.2451215525288717e-06\n",
      "fps: 8.338344459597545\n",
      "TIMESTEP 1054 / STATE explore / EPSILON 0.09904795299999855 / ACTION 0 / REWARD 0.1 / Q_MAX  9.94905 / Loss  3.0547369078703923e-06\n",
      "fps: 8.626172798881187\n",
      "TIMESTEP 1055 / STATE explore / EPSILON 0.09904695399999855 / ACTION 0 / REWARD 0.1 / Q_MAX  9.967573 / Loss  2.8388667487888597e-06\n",
      "fps: 8.934011536265965\n",
      "TIMESTEP 1056 / STATE explore / EPSILON 0.09904595499999855 / ACTION 0 / REWARD 0.1 / Q_MAX  9.988819 / Loss  2.3144951910580858e-07\n",
      "fps: 10.644928290260115\n",
      "TIMESTEP 1057 / STATE explore / EPSILON 0.09904495599999855 / ACTION 0 / REWARD 0.1 / Q_MAX  10.012117 / Loss  5.1316837925696746e-06\n",
      "fps: 10.107146299616371\n",
      "TIMESTEP 1058 / STATE explore / EPSILON 0.09904395699999854 / ACTION 0 / REWARD 0.1 / Q_MAX  10.036988 / Loss  7.299391654669307e-06\n",
      "fps: 8.479765478898155\n",
      "TIMESTEP 1059 / STATE explore / EPSILON 0.09904295799999854 / ACTION 0 / REWARD 0.1 / Q_MAX  10.064448 / Loss  1.3929288797953632e-06\n",
      "fps: 7.523455731600349\n",
      "TIMESTEP 1060 / STATE explore / EPSILON 0.09904195899999854 / ACTION 0 / REWARD 0.1 / Q_MAX  9.682025 / Loss  3.1175748063105857e-06\n",
      "fps: 8.933611929362701\n",
      "TIMESTEP 1061 / STATE explore / EPSILON 0.09904095999999854 / ACTION 0 / REWARD 0.1 / Q_MAX  10.123348 / Loss  2.598301762191113e-06\n",
      "fps: 9.43986964291341\n",
      "TIMESTEP 1062 / STATE explore / EPSILON 0.09903996099999854 / ACTION 0 / REWARD 0.1 / Q_MAX  10.154246 / Loss  9.476469813307631e-07\n",
      "fps: 9.264982715013089\n",
      "TIMESTEP 1063 / STATE explore / EPSILON 0.09903896199999854 / ACTION 0 / REWARD 0.1 / Q_MAX  10.180667 / Loss  2.318516862942488e-06\n",
      "fps: 8.479679760629157\n",
      "TIMESTEP 1064 / STATE explore / EPSILON 0.09903796299999854 / ACTION 0 / REWARD 0.1 / Q_MAX  10.204064 / Loss  1.0998669495165814e-06\n",
      "fps: 10.426433592856645\n",
      "TIMESTEP 1065 / STATE explore / EPSILON 0.09903696399999853 / ACTION 0 / REWARD 0.1 / Q_MAX  10.223399 / Loss  2.5386555080331163e-06\n",
      "fps: 10.75920520016212\n",
      "TIMESTEP 1066 / STATE explore / EPSILON 0.09903596499999853 / ACTION 0 / REWARD 0.1 / Q_MAX  9.905568 / Loss  1.9838230400637258e-06\n",
      "fps: 10.006140662685812\n",
      "TIMESTEP 1067 / STATE explore / EPSILON 0.09903496599999853 / ACTION 0 / REWARD 0.1 / Q_MAX  10.256481 / Loss  9.095876885112375e-05\n",
      "fps: 11.111916494462989\n",
      "TIMESTEP 1068 / STATE explore / EPSILON 0.09903396699999853 / ACTION 0 / REWARD 0.1 / Q_MAX  10.2602825 / Loss  4.037716280436143e-06\n",
      "fps: 10.31570559329454\n",
      "TIMESTEP 1069 / STATE explore / EPSILON 0.09903296799999853 / ACTION 0 / REWARD 0.1 / Q_MAX  10.257147 / Loss  2.289483745698817e-06\n",
      "fps: 10.876216160149362\n",
      "TIMESTEP 1070 / STATE explore / EPSILON 0.09903196899999853 / ACTION 0 / REWARD 0.1 / Q_MAX  10.251343 / Loss  0.0005300184129737318\n",
      "fps: 9.809975324812012\n",
      "TIMESTEP 1071 / STATE explore / EPSILON 0.09903096999999852 / ACTION 0 / REWARD 0.1 / Q_MAX  10.23234 / Loss  4.8268793761963025e-06\n",
      "fps: 9.71468144685917\n",
      "TIMESTEP 1072 / STATE explore / EPSILON 0.09902997099999852 / ACTION 0 / REWARD 0.1 / Q_MAX  10.217504 / Loss  2.6693141990108415e-05\n",
      "fps: 10.261394606919701\n",
      "TIMESTEP 1073 / STATE explore / EPSILON 0.09902897199999852 / ACTION 0 / REWARD 0.1 / Q_MAX  10.210088 / Loss  3.6705507682199823e-06\n",
      "fps: 10.995857845450447\n",
      "TIMESTEP 1074 / STATE explore / EPSILON 0.09902797299999852 / ACTION 0 / REWARD 0.1 / Q_MAX  10.206324 / Loss  2.750916337390663e-06\n",
      "fps: 9.407348593934337\n",
      "TIMESTEP 1075 / STATE explore / EPSILON 0.09902697399999852 / ACTION 0 / REWARD 0.1 / Q_MAX  10.206535 / Loss  1.2722970495815389e-05\n",
      "fps: 10.31557873974112\n",
      "TIMESTEP 1076 / STATE explore / EPSILON 0.09902597499999852 / ACTION 0 / REWARD 0.1 / Q_MAX  10.209244 / Loss  1.7262154869968072e-06\n",
      "fps: 9.713624026160502\n",
      "TIMESTEP 1077 / STATE explore / EPSILON 0.09902497599999852 / ACTION 0 / REWARD 0.1 / Q_MAX  10.21399 / Loss  6.225712422747165e-05\n",
      "fps: 10.532683103224867\n",
      "TIMESTEP 1078 / STATE explore / EPSILON 0.09902397699999851 / ACTION 0 / REWARD 0.1 / Q_MAX  10.216126 / Loss  2.8784321330022067e-05\n",
      "fps: 4.134743026700381\n",
      "TIMESTEP 1079 / STATE explore / EPSILON 0.09902297799999851 / ACTION 0 / REWARD 0.1 / Q_MAX  10.223447 / Loss  1.1416381312301382e-05\n",
      "fps: 3.6923737848137703\n",
      "TIMESTEP 1080 / STATE explore / EPSILON 0.09902197899999851 / ACTION 0 / REWARD 0.1 / Q_MAX  10.2364435 / Loss  1.0836923820534139e-06\n",
      "fps: 4.978225107711298\n",
      "TIMESTEP 1081 / STATE explore / EPSILON 0.09902097999999851 / ACTION 0 / REWARD 0.1 / Q_MAX  10.255921 / Loss  1.1421778253861703e-05\n",
      "fps: 6.806912881400493\n",
      "TIMESTEP 1082 / STATE explore / EPSILON 0.09901998099999851 / ACTION 0 / REWARD 0.1 / Q_MAX  10.274262 / Loss  1.7666814073891146e-06\n",
      "----------Random Action----------\n",
      "fps: 5.749764899091674\n",
      "TIMESTEP 1083 / STATE explore / EPSILON 0.0990189819999985 / ACTION 1 / REWARD 0.1 / Q_MAX  10.223572 / Loss  0.026183046400547028\n",
      "fps: 8.067955188969593\n",
      "TIMESTEP 1084 / STATE explore / EPSILON 0.0990179829999985 / ACTION 0 / REWARD 0.1 / Q_MAX  10.484473 / Loss  2.4322634999407455e-05\n",
      "fps: 7.530736608505489\n",
      "TIMESTEP 1085 / STATE explore / EPSILON 0.0990169839999985 / ACTION 0 / REWARD 0.1 / Q_MAX  10.641803 / Loss  2.0779843907803297e-05\n",
      "fps: 8.478685555339931\n",
      "TIMESTEP 1086 / STATE explore / EPSILON 0.0990159849999985 / ACTION 0 / REWARD 0.1 / Q_MAX  10.786026 / Loss  3.250506051699631e-05\n",
      "fps: 7.196840071517061\n",
      "TIMESTEP 1087 / STATE explore / EPSILON 0.0990149859999985 / ACTION 0 / REWARD 0.1 / Q_MAX  10.921629 / Loss  4.01824145228602e-05\n",
      "fps: 9.094525033066631\n",
      "TIMESTEP 1088 / STATE explore / EPSILON 0.0990139869999985 / ACTION 0 / REWARD 0.1 / Q_MAX  11.032158 / Loss  4.5658489398192614e-05\n",
      "fps: 6.454721731475953\n",
      "TIMESTEP 1089 / STATE explore / EPSILON 0.0990129879999985 / ACTION 1 / REWARD 0.1 / Q_MAX  11.102707 / Loss  4.950392758473754e-05\n",
      "fps: 8.069476349336442\n",
      "TIMESTEP 1090 / STATE explore / EPSILON 0.0990119889999985 / ACTION 0 / REWARD 0.1 / Q_MAX  11.134055 / Loss  3.825376552413218e-05\n",
      "fps: 8.0706098314223\n",
      "TIMESTEP 1091 / STATE explore / EPSILON 0.0990109899999985 / ACTION 0 / REWARD 0.1 / Q_MAX  10.965274 / Loss  3.988346361438744e-05\n",
      "fps: 7.580400824855551\n",
      "TIMESTEP 1092 / STATE explore / EPSILON 0.09900999099999849 / ACTION 0 / REWARD 0.1 / Q_MAX  10.944207 / Loss  0.002614123746752739\n",
      "----------Random Action----------\n",
      "fps: 5.528233597730873\n",
      "TIMESTEP 1093 / STATE explore / EPSILON 0.09900899199999849 / ACTION 0 / REWARD 0.1 / Q_MAX  11.147495 / Loss  0.0002453429624438286\n",
      "fps: 5.851591841290214\n",
      "TIMESTEP 1094 / STATE explore / EPSILON 0.09900799299999849 / ACTION 0 / REWARD 0.1 / Q_MAX  11.186537 / Loss  7.719345012446865e-05\n",
      "----------Random Action----------\n",
      "fps: 5.919935639127458\n",
      "TIMESTEP 1095 / STATE explore / EPSILON 0.09900699399999849 / ACTION 0 / REWARD 0.1 / Q_MAX  11.206893 / Loss  3.9412341720890254e-05\n",
      "fps: 8.00494690474918\n",
      "TIMESTEP 1096 / STATE explore / EPSILON 0.09900599499999849 / ACTION 1 / REWARD 0.1 / Q_MAX  11.193371 / Loss  0.07273276150226593\n",
      "----------Random Action----------\n",
      "fps: 6.539958056242058\n",
      "TIMESTEP 1097 / STATE explore / EPSILON 0.09900499599999849 / ACTION 1 / REWARD 0.1 / Q_MAX  11.482879 / Loss  0.01056408416479826\n",
      "fps: 10.190936215291613\n",
      "TIMESTEP 1098 / STATE explore / EPSILON 0.09900399699999848 / ACTION 0 / REWARD 0.1 / Q_MAX  11.756153 / Loss  0.0001653895014896989\n",
      "fps: 7.300815668635922\n",
      "TIMESTEP 1099 / STATE explore / EPSILON 0.09900299799999848 / ACTION 0 / REWARD 0.1 / Q_MAX  12.137442 / Loss  0.0026903951074928045\n",
      "fps: 8.931386202285282\n",
      "TIMESTEP 1100 / STATE explore / EPSILON 0.09900199899999848 / ACTION 0 / REWARD 0.1 / Q_MAX  12.45588 / Loss  0.0005776580655947328\n",
      "fps: 6.582284361067213\n",
      "TIMESTEP 1101 / STATE explore / EPSILON 0.09900099999999848 / ACTION 1 / REWARD 0.1 / Q_MAX  12.829929 / Loss  0.5267074704170227\n",
      "fps: 8.005053849838824\n",
      "TIMESTEP 1102 / STATE explore / EPSILON 0.09900000099999848 / ACTION 1 / REWARD 0.1 / Q_MAX  12.876902 / Loss  0.05595117062330246\n",
      "fps: 6.178843488009287\n",
      "TIMESTEP 1103 / STATE explore / EPSILON 0.09899900199999848 / ACTION 0 / REWARD 0.1 / Q_MAX  13.459642 / Loss  0.046097733080387115\n",
      "fps: 8.855486984813329\n",
      "TIMESTEP 1104 / STATE explore / EPSILON 0.09899800299999847 / ACTION 0 / REWARD 0.1 / Q_MAX  14.273426 / Loss  0.00039270089473575354\n",
      "fps: 8.001480376427676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 1105 / STATE explore / EPSILON 0.09899700399999847 / ACTION 0 / REWARD 0.1 / Q_MAX  16.870167 / Loss  0.03344963863492012\n",
      "fps: 7.143338391553963\n",
      "TIMESTEP 1106 / STATE explore / EPSILON 0.09899600499999847 / ACTION 1 / REWARD 0.1 / Q_MAX  16.30776 / Loss  0.0014885237906128168\n",
      "fps: 7.302760535914825\n",
      "TIMESTEP 1107 / STATE explore / EPSILON 0.09899500599999847 / ACTION 1 / REWARD 0.1 / Q_MAX  17.926397 / Loss  0.02891041338443756\n",
      "fps: 6.668199255645063\n",
      "TIMESTEP 1108 / STATE explore / EPSILON 0.09899400699999847 / ACTION 1 / REWARD 0.1 / Q_MAX  19.142052 / Loss  0.019689207896590233\n",
      "----------Random Action----------\n",
      "fps: 7.284926991008266\n",
      "TIMESTEP 1109 / STATE explore / EPSILON 0.09899300799999847 / ACTION 0 / REWARD 0.1 / Q_MAX  20.437443 / Loss  0.004142214078456163\n",
      "fps: 8.812840516797603\n",
      "TIMESTEP 1110 / STATE explore / EPSILON 0.09899200899999847 / ACTION 0 / REWARD 0.1 / Q_MAX  21.604628 / Loss  0.06632880866527557\n",
      "fps: 6.658418065642735\n",
      "TIMESTEP 1111 / STATE explore / EPSILON 0.09899100999999846 / ACTION 1 / REWARD 0.1 / Q_MAX  22.748217 / Loss  0.004270918667316437\n",
      "fps: 7.940193511624517\n",
      "TIMESTEP 1112 / STATE explore / EPSILON 0.09899001099999846 / ACTION 1 / REWARD 0.1 / Q_MAX  23.742117 / Loss  0.014606486074626446\n",
      "----------Random Action----------\n",
      "fps: 8.003709605091165\n",
      "TIMESTEP 1113 / STATE explore / EPSILON 0.09898901199999846 / ACTION 0 / REWARD 0.1 / Q_MAX  24.339754 / Loss  1.060289978981018\n",
      "fps: 7.941381177602157\n",
      "TIMESTEP 1114 / STATE explore / EPSILON 0.09898801299999846 / ACTION 1 / REWARD 0.1 / Q_MAX  24.139896 / Loss  0.08410118520259857\n",
      "fps: 6.582315350717505\n",
      "TIMESTEP 1115 / STATE explore / EPSILON 0.09898701399999846 / ACTION 1 / REWARD 0.1 / Q_MAX  24.220873 / Loss  0.013060277327895164\n",
      "fps: 7.7565858701300625\n",
      "TIMESTEP 1116 / STATE explore / EPSILON 0.09898601499999846 / ACTION 1 / REWARD 0.1 / Q_MAX  24.718636 / Loss  0.02936805970966816\n",
      "fps: 6.7606120537779475\n",
      "TIMESTEP 1117 / STATE explore / EPSILON 0.09898501599999845 / ACTION 1 / REWARD 0.1 / Q_MAX  25.525822 / Loss  0.8009189367294312\n",
      "fps: 6.625340009793546\n",
      "TIMESTEP 1118 / STATE explore / EPSILON 0.09898401699999845 / ACTION 0 / REWARD 0.1 / Q_MAX  25.29261 / Loss  0.3841663897037506\n",
      "fps: 6.668994444497445\n",
      "TIMESTEP 1119 / STATE explore / EPSILON 0.09898301799999845 / ACTION 1 / REWARD 0.1 / Q_MAX  25.50339 / Loss  0.04411779344081879\n",
      "fps: 6.714646602097174\n",
      "TIMESTEP 1120 / STATE explore / EPSILON 0.09898201899999845 / ACTION 1 / REWARD 0.1 / Q_MAX  25.634716 / Loss  0.029031990095973015\n",
      "fps: 6.612648888118117\n",
      "TIMESTEP 1121 / STATE explore / EPSILON 0.09898101999999845 / ACTION 1 / REWARD 0.1 / Q_MAX  25.800524 / Loss  0.02529013529419899\n",
      "fps: 9.902362323699258\n",
      "TIMESTEP 1122 / STATE explore / EPSILON 0.09898002099999845 / ACTION 0 / REWARD 0.1 / Q_MAX  25.558445 / Loss  0.10666100680828094\n",
      "fps: 7.9414413220386475\n",
      "TIMESTEP 1123 / STATE explore / EPSILON 0.09897902199999845 / ACTION 0 / REWARD 0.1 / Q_MAX  26.880806 / Loss  0.06296877562999725\n",
      "fps: 10.17573091824827\n",
      "TIMESTEP 1124 / STATE explore / EPSILON 0.09897802299999844 / ACTION 0 / REWARD 0.1 / Q_MAX  27.510187 / Loss  0.01844138652086258\n",
      "fps: 5.920779668098994\n",
      "TIMESTEP 1125 / STATE explore / EPSILON 0.09897702399999844 / ACTION 1 / REWARD 0.1 / Q_MAX  28.110046 / Loss  0.018384525552392006\n",
      "fps: 4.221996072237808\n",
      "TIMESTEP 1126 / STATE explore / EPSILON 0.09897602499999844 / ACTION 1 / REWARD 0.1 / Q_MAX  28.909622 / Loss  0.07326578348875046\n",
      "----------Random Action----------\n",
      "fps: 5.750718787747497\n",
      "TIMESTEP 1127 / STATE explore / EPSILON 0.09897502599999844 / ACTION 1 / REWARD 0.1 / Q_MAX  30.42685 / Loss  0.014650896191596985\n",
      "fps: 9.351535623111825\n",
      "TIMESTEP 1128 / STATE explore / EPSILON 0.09897402699999844 / ACTION 0 / REWARD -1 / Q_MAX  31.494854 / Loss  0.04206818342208862\n",
      "fps: 7.754606852919045\n",
      "TIMESTEP 1129 / STATE explore / EPSILON 0.09897302799999844 / ACTION 0 / REWARD 0.1 / Q_MAX  32.55421 / Loss  0.3646411895751953\n",
      "fps: 8.910703966824162\n",
      "TIMESTEP 1130 / STATE explore / EPSILON 0.09897202899999843 / ACTION 0 / REWARD 0.1 / Q_MAX  33.423355 / Loss  0.015598895028233528\n",
      "fps: 7.877426067620003\n",
      "TIMESTEP 1131 / STATE explore / EPSILON 0.09897102999999843 / ACTION 0 / REWARD 0.1 / Q_MAX  33.62835 / Loss  0.011829917319118977\n",
      "fps: 9.97271374156518\n",
      "TIMESTEP 1132 / STATE explore / EPSILON 0.09897003099999843 / ACTION 0 / REWARD 0.1 / Q_MAX  33.285656 / Loss  0.025147605687379837\n",
      "fps: 6.213433910387267\n",
      "TIMESTEP 1133 / STATE explore / EPSILON 0.09896903199999843 / ACTION 1 / REWARD 0.1 / Q_MAX  32.31318 / Loss  0.018901338800787926\n",
      "fps: 7.11340255141325\n",
      "TIMESTEP 1134 / STATE explore / EPSILON 0.09896803299999843 / ACTION 1 / REWARD 0.1 / Q_MAX  31.199211 / Loss  0.036870598793029785\n",
      "fps: 8.22681937026438\n",
      "TIMESTEP 1135 / STATE explore / EPSILON 0.09896703399999843 / ACTION 0 / REWARD 0.1 / Q_MAX  29.662508 / Loss  0.6394436955451965\n",
      "fps: 9.882297502980496\n",
      "TIMESTEP 1136 / STATE explore / EPSILON 0.09896603499999843 / ACTION 0 / REWARD 0.1 / Q_MAX  28.546467 / Loss  0.05170074850320816\n",
      "fps: 8.003343071236802\n",
      "TIMESTEP 1137 / STATE explore / EPSILON 0.09896503599999842 / ACTION 0 / REWARD 0.1 / Q_MAX  27.844648 / Loss  0.016018806025385857\n",
      "----------Random Action----------\n",
      "fps: 6.622630575893414\n",
      "TIMESTEP 1138 / STATE explore / EPSILON 0.09896403699999842 / ACTION 1 / REWARD 0.1 / Q_MAX  27.061852 / Loss  0.2477031797170639\n",
      "fps: 10.050787062885954\n",
      "TIMESTEP 1139 / STATE explore / EPSILON 0.09896303799999842 / ACTION 0 / REWARD 0.1 / Q_MAX  26.13797 / Loss  0.11131737381219864\n",
      "fps: 7.045473099296344\n",
      "TIMESTEP 1140 / STATE explore / EPSILON 0.09896203899999842 / ACTION 0 / REWARD 0.1 / Q_MAX  25.805088 / Loss  0.04886883869767189\n",
      "fps: 7.467333230072977\n",
      "TIMESTEP 1141 / STATE explore / EPSILON 0.09896103999999842 / ACTION 1 / REWARD 0.1 / Q_MAX  25.591734 / Loss  0.011316216550767422\n",
      "fps: 7.043757473163785\n",
      "TIMESTEP 1142 / STATE explore / EPSILON 0.09896004099999842 / ACTION 0 / REWARD 0.1 / Q_MAX  25.632923 / Loss  0.07213650643825531\n",
      "----------Random Action----------\n",
      "fps: 7.333365970332967\n",
      "TIMESTEP 1143 / STATE explore / EPSILON 0.09895904199999841 / ACTION 1 / REWARD 0.1 / Q_MAX  25.867573 / Loss  0.033519402146339417\n",
      "fps: 8.041211577431792\n",
      "TIMESTEP 1144 / STATE explore / EPSILON 0.09895804299999841 / ACTION 0 / REWARD 0.1 / Q_MAX  26.051573 / Loss  0.1437719464302063\n",
      "fps: 9.907016843542364\n",
      "TIMESTEP 1145 / STATE explore / EPSILON 0.09895704399999841 / ACTION 0 / REWARD 0.1 / Q_MAX  26.50582 / Loss  0.0385124534368515\n",
      "fps: 8.003633241102948\n",
      "TIMESTEP 1146 / STATE explore / EPSILON 0.09895604499999841 / ACTION 0 / REWARD 0.1 / Q_MAX  26.830954 / Loss  0.4868046045303345\n",
      "fps: 7.369559984467767\n",
      "TIMESTEP 1147 / STATE explore / EPSILON 0.09895504599999841 / ACTION 0 / REWARD 0.1 / Q_MAX  27.345524 / Loss  0.02546924538910389\n",
      "----------Random Action----------\n",
      "fps: 7.067614339106316\n",
      "TIMESTEP 1148 / STATE explore / EPSILON 0.09895404699999841 / ACTION 1 / REWARD 0.1 / Q_MAX  27.483162 / Loss  0.00827824417501688\n",
      "fps: 6.625842387784311\n",
      "TIMESTEP 1149 / STATE explore / EPSILON 0.0989530479999984 / ACTION 0 / REWARD 0.1 / Q_MAX  27.5844 / Loss  0.4874635934829712\n",
      "fps: 6.452596697322228\n",
      "TIMESTEP 1150 / STATE explore / EPSILON 0.0989520489999984 / ACTION 1 / REWARD 0.1 / Q_MAX  28.19259 / Loss  0.7878280282020569\n",
      "fps: 5.349529558663914\n",
      "TIMESTEP 1151 / STATE explore / EPSILON 0.0989510499999984 / ACTION 1 / REWARD 0.1 / Q_MAX  29.606934 / Loss  0.3062511384487152\n",
      "fps: 6.101338298615152\n",
      "TIMESTEP 1152 / STATE explore / EPSILON 0.0989500509999984 / ACTION 1 / REWARD 0.1 / Q_MAX  31.732626 / Loss  0.08464032411575317\n",
      "----------Random Action----------\n",
      "fps: 6.80681346073951\n",
      "TIMESTEP 1153 / STATE explore / EPSILON 0.0989490519999984 / ACTION 1 / REWARD 0.1 / Q_MAX  34.044636 / Loss  0.15895643830299377\n",
      "fps: 7.81731837328065\n",
      "TIMESTEP 1154 / STATE explore / EPSILON 0.0989480529999984 / ACTION 0 / REWARD 0.1 / Q_MAX  36.552364 / Loss  0.08866152167320251\n",
      "fps: 5.322065684807835\n",
      "TIMESTEP 1155 / STATE explore / EPSILON 0.0989470539999984 / ACTION 1 / REWARD 0.1 / Q_MAX  38.916527 / Loss  0.08840584009885788\n",
      "fps: 5.002831631253384\n",
      "TIMESTEP 1156 / STATE explore / EPSILON 0.0989460549999984 / ACTION 1 / REWARD 0.1 / Q_MAX  40.934593 / Loss  1.2353862524032593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 3.976382342122328\n",
      "TIMESTEP 1157 / STATE explore / EPSILON 0.0989450559999984 / ACTION 1 / REWARD 0.1 / Q_MAX  42.603184 / Loss  0.24108365178108215\n",
      "fps: 5.003118074094219\n",
      "TIMESTEP 1158 / STATE explore / EPSILON 0.09894405699999839 / ACTION 1 / REWARD 0.1 / Q_MAX  42.789944 / Loss  0.4767744243144989\n",
      "----------Random Action----------\n",
      "fps: 7.358725001184259\n",
      "TIMESTEP 1159 / STATE explore / EPSILON 0.09894305799999839 / ACTION 0 / REWARD 0.1 / Q_MAX  43.153866 / Loss  0.033835556358098984\n",
      "fps: 9.815645857320714\n",
      "TIMESTEP 1160 / STATE explore / EPSILON 0.09894205899999839 / ACTION 0 / REWARD 0.1 / Q_MAX  42.74349 / Loss  0.38706278800964355\n",
      "fps: 6.715614192391444\n",
      "TIMESTEP 1161 / STATE explore / EPSILON 0.09894105999999839 / ACTION 1 / REWARD 0.1 / Q_MAX  42.33318 / Loss  0.27472642064094543\n",
      "fps: 6.6842243658494995\n",
      "TIMESTEP 1162 / STATE explore / EPSILON 0.09894006099999839 / ACTION 1 / REWARD 0.1 / Q_MAX  41.63404 / Loss  0.9442819356918335\n",
      "fps: 6.581468405278602\n",
      "TIMESTEP 1163 / STATE explore / EPSILON 0.09893906199999838 / ACTION 1 / REWARD 0.1 / Q_MAX  41.522816 / Loss  0.6555044651031494\n",
      "fps: 8.073297300626722\n",
      "TIMESTEP 1164 / STATE explore / EPSILON 0.09893806299999838 / ACTION 0 / REWARD 0.1 / Q_MAX  41.791203 / Loss  0.04543159157037735\n",
      "fps: 5.963171483490861\n",
      "TIMESTEP 1165 / STATE explore / EPSILON 0.09893706399999838 / ACTION 1 / REWARD 0.1 / Q_MAX  40.556206 / Loss  0.032272811979055405\n",
      "fps: 7.216354508260183\n",
      "TIMESTEP 1166 / STATE explore / EPSILON 0.09893606499999838 / ACTION 1 / REWARD 0.1 / Q_MAX  40.465603 / Loss  0.07959914952516556\n",
      "fps: 6.714033939110655\n",
      "TIMESTEP 1167 / STATE explore / EPSILON 0.09893506599999838 / ACTION 1 / REWARD 0.1 / Q_MAX  39.28489 / Loss  0.05596768110990524\n",
      "fps: 5.825624500850724\n",
      "TIMESTEP 1168 / STATE explore / EPSILON 0.09893406699999838 / ACTION 1 / REWARD 0.1 / Q_MAX  37.748608 / Loss  0.7755453586578369\n",
      "fps: 7.956158937131172\n",
      "TIMESTEP 1169 / STATE explore / EPSILON 0.09893306799999838 / ACTION 1 / REWARD 0.1 / Q_MAX  36.4786 / Loss  0.028977222740650177\n",
      "fps: 6.626617036470611\n",
      "TIMESTEP 1170 / STATE explore / EPSILON 0.09893206899999837 / ACTION 1 / REWARD 0.1 / Q_MAX  35.043926 / Loss  0.24378536641597748\n",
      "fps: 6.715549677696192\n",
      "TIMESTEP 1171 / STATE explore / EPSILON 0.09893106999999837 / ACTION 1 / REWARD 0.1 / Q_MAX  33.827038 / Loss  0.17844977974891663\n",
      "fps: 6.536665949773867\n",
      "TIMESTEP 1172 / STATE explore / EPSILON 0.09893007099999837 / ACTION 1 / REWARD 0.1 / Q_MAX  32.986774 / Loss  0.09502077847719193\n",
      "fps: 6.780864926036699\n",
      "TIMESTEP 1173 / STATE explore / EPSILON 0.09892907199999837 / ACTION 1 / REWARD -1 / Q_MAX  32.288128 / Loss  0.016035210341215134\n",
      "----------Random Action----------\n",
      "fps: 9.907344461950192\n",
      "TIMESTEP 1174 / STATE explore / EPSILON 0.09892807299999837 / ACTION 0 / REWARD 0.1 / Q_MAX  31.418293 / Loss  0.8908476233482361\n",
      "fps: 6.373334224785824\n",
      "TIMESTEP 1175 / STATE explore / EPSILON 0.09892707399999837 / ACTION 1 / REWARD 0.1 / Q_MAX  30.85694 / Loss  0.4371820092201233\n",
      "fps: 6.806957069293176\n",
      "TIMESTEP 1176 / STATE explore / EPSILON 0.09892607499999836 / ACTION 1 / REWARD 0.1 / Q_MAX  30.29939 / Loss  0.15942780673503876\n",
      "fps: 7.357395453269716\n",
      "TIMESTEP 1177 / STATE explore / EPSILON 0.09892507599999836 / ACTION 1 / REWARD 0.1 / Q_MAX  29.923296 / Loss  2.1464991569519043\n",
      "fps: 7.39475317348378\n",
      "TIMESTEP 1178 / STATE explore / EPSILON 0.09892407699999836 / ACTION 1 / REWARD 0.1 / Q_MAX  29.15685 / Loss  0.8282660245895386\n",
      "fps: 8.201899949548478\n",
      "TIMESTEP 1179 / STATE explore / EPSILON 0.09892307799999836 / ACTION 0 / REWARD 0.1 / Q_MAX  29.379171 / Loss  0.21809281408786774\n",
      "----------Random Action----------\n",
      "fps: 7.697029866495389\n",
      "TIMESTEP 1180 / STATE explore / EPSILON 0.09892207899999836 / ACTION 1 / REWARD 0.1 / Q_MAX  29.931892 / Loss  0.5102395415306091\n",
      "fps: 7.367307614616213\n",
      "TIMESTEP 1181 / STATE explore / EPSILON 0.09892107999999836 / ACTION 1 / REWARD 0.1 / Q_MAX  30.672638 / Loss  0.1992519646883011\n",
      "fps: 6.715592687355299\n",
      "TIMESTEP 1182 / STATE explore / EPSILON 0.09892008099999836 / ACTION 1 / REWARD 0.1 / Q_MAX  31.322092 / Loss  0.03006548248231411\n",
      "fps: 10.006188405209329\n",
      "TIMESTEP 1183 / STATE explore / EPSILON 0.09891908199999835 / ACTION 0 / REWARD 0.1 / Q_MAX  31.80035 / Loss  0.06414689868688583\n",
      "fps: 7.465924281719423\n",
      "TIMESTEP 1184 / STATE explore / EPSILON 0.09891808299999835 / ACTION 0 / REWARD 0.1 / Q_MAX  32.130146 / Loss  90.52928924560547\n",
      "fps: 8.069491874337452\n",
      "TIMESTEP 1185 / STATE explore / EPSILON 0.09891708399999835 / ACTION 1 / REWARD 0.1 / Q_MAX  30.854576 / Loss  0.6762392520904541\n",
      "fps: 6.6706914814096185\n",
      "TIMESTEP 1186 / STATE explore / EPSILON 0.09891608499999835 / ACTION 1 / REWARD 0.1 / Q_MAX  40.69061 / Loss  1.478011965751648\n",
      "fps: 8.408654231688345\n",
      "TIMESTEP 1187 / STATE explore / EPSILON 0.09891508599999835 / ACTION 1 / REWARD 0.1 / Q_MAX  29.850487 / Loss  1.3280317783355713\n",
      "fps: 5.717831689269142\n",
      "TIMESTEP 1188 / STATE explore / EPSILON 0.09891408699999835 / ACTION 1 / REWARD 0.1 / Q_MAX  30.078299 / Loss  1.2626917362213135\n",
      "fps: 5.0536035943770985\n",
      "TIMESTEP 1189 / STATE explore / EPSILON 0.09891308799999834 / ACTION 1 / REWARD 0.1 / Q_MAX  30.341068 / Loss  0.6236257553100586\n",
      "fps: 5.62147209169546\n",
      "TIMESTEP 1190 / STATE explore / EPSILON 0.09891208899999834 / ACTION 1 / REWARD 0.1 / Q_MAX  30.843092 / Loss  0.433289110660553\n",
      "fps: 7.276434146451478\n",
      "TIMESTEP 1191 / STATE explore / EPSILON 0.09891108999999834 / ACTION 1 / REWARD 0.1 / Q_MAX  31.634243 / Loss  0.3867991268634796\n",
      "fps: 6.82933437540706\n",
      "TIMESTEP 1192 / STATE explore / EPSILON 0.09891009099999834 / ACTION 1 / REWARD 0.1 / Q_MAX  32.818874 / Loss  0.018561797216534615\n",
      "fps: 9.8099064922186\n",
      "TIMESTEP 1193 / STATE explore / EPSILON 0.09890909199999834 / ACTION 0 / REWARD 0.1 / Q_MAX  34.24646 / Loss  0.12482072412967682\n",
      "----------Random Action----------\n",
      "fps: 9.351577323288288\n",
      "TIMESTEP 1194 / STATE explore / EPSILON 0.09890809299999834 / ACTION 0 / REWARD 0.1 / Q_MAX  35.417057 / Loss  2.106340169906616\n",
      "fps: 7.096552820988302\n",
      "TIMESTEP 1195 / STATE explore / EPSILON 0.09890709399999834 / ACTION 1 / REWARD 0.1 / Q_MAX  36.1809 / Loss  0.5679104924201965\n",
      "fps: 7.198717574650817\n",
      "TIMESTEP 1196 / STATE explore / EPSILON 0.09890609499999833 / ACTION 1 / REWARD 0.1 / Q_MAX  36.98768 / Loss  1.128365397453308\n",
      "fps: 6.539998846149648\n",
      "TIMESTEP 1197 / STATE explore / EPSILON 0.09890509599999833 / ACTION 1 / REWARD 0.1 / Q_MAX  38.101685 / Loss  0.21587832272052765\n",
      "fps: 6.948180666108953\n",
      "TIMESTEP 1198 / STATE explore / EPSILON 0.09890409699999833 / ACTION 1 / REWARD 0.1 / Q_MAX  41.71934 / Loss  0.12567856907844543\n",
      "fps: 7.580428225198669\n",
      "TIMESTEP 1199 / STATE explore / EPSILON 0.09890309799999833 / ACTION 1 / REWARD 0.1 / Q_MAX  40.391727 / Loss  0.17240075767040253\n",
      "fps: 6.716205634862812\n",
      "TIMESTEP 1200 / STATE explore / EPSILON 0.09890209899999833 / ACTION 1 / REWARD 0.1 / Q_MAX  41.50316 / Loss  0.25934767723083496\n",
      "fps: 7.302925833838268\n",
      "TIMESTEP 1201 / STATE explore / EPSILON 0.09890109999999833 / ACTION 1 / REWARD 0.1 / Q_MAX  42.581905 / Loss  0.13699468970298767\n",
      "fps: 7.528195481974204\n",
      "TIMESTEP 1202 / STATE explore / EPSILON 0.09890010099999832 / ACTION 1 / REWARD 0.1 / Q_MAX  43.4007 / Loss  0.0674339085817337\n",
      "----------Random Action----------\n",
      "fps: 8.003617968480167\n",
      "TIMESTEP 1203 / STATE explore / EPSILON 0.09889910199999832 / ACTION 0 / REWARD 0.1 / Q_MAX  46.214428 / Loss  0.3490464687347412\n",
      "fps: 7.303790591087026\n",
      "TIMESTEP 1204 / STATE explore / EPSILON 0.09889810299999832 / ACTION 1 / REWARD 0.1 / Q_MAX  45.34332 / Loss  0.035779472440481186\n",
      "fps: 7.397518113258129\n",
      "TIMESTEP 1205 / STATE explore / EPSILON 0.09889710399999832 / ACTION 1 / REWARD 0.1 / Q_MAX  46.109444 / Loss  0.22413980960845947\n",
      "fps: 6.7609280853162765\n",
      "TIMESTEP 1206 / STATE explore / EPSILON 0.09889610499999832 / ACTION 1 / REWARD 0.1 / Q_MAX  49.653744 / Loss  0.43510934710502625\n",
      "fps: 7.303752435713701\n",
      "TIMESTEP 1207 / STATE explore / EPSILON 0.09889510599999832 / ACTION 1 / REWARD 0.1 / Q_MAX  46.790756 / Loss  0.14835400879383087\n",
      "fps: 7.941396213625873\n",
      "TIMESTEP 1208 / STATE explore / EPSILON 0.09889410699999832 / ACTION 1 / REWARD 0.1 / Q_MAX  46.838116 / Loss  0.053868409246206284\n",
      "fps: 6.625748186107789\n",
      "TIMESTEP 1209 / STATE explore / EPSILON 0.09889310799999831 / ACTION 1 / REWARD 0.1 / Q_MAX  46.83307 / Loss  0.04732990264892578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 7.636893317019169\n",
      "TIMESTEP 1210 / STATE explore / EPSILON 0.09889210899999831 / ACTION 1 / REWARD 0.1 / Q_MAX  46.563145 / Loss  0.5132743120193481\n",
      "fps: 6.373431070427843\n",
      "TIMESTEP 1211 / STATE explore / EPSILON 0.09889110999999831 / ACTION 1 / REWARD 0.1 / Q_MAX  45.943214 / Loss  0.08191686868667603\n",
      "fps: 6.625277217901168\n",
      "TIMESTEP 1212 / STATE explore / EPSILON 0.09889011099999831 / ACTION 1 / REWARD 0.1 / Q_MAX  45.379433 / Loss  33.66136169433594\n",
      "fps: 7.82074379317739\n",
      "TIMESTEP 1213 / STATE explore / EPSILON 0.09888911199999831 / ACTION 1 / REWARD 0.1 / Q_MAX  44.06281 / Loss  0.08411271870136261\n",
      "fps: 6.900251378634132\n",
      "TIMESTEP 1214 / STATE explore / EPSILON 0.0988881129999983 / ACTION 1 / REWARD -1 / Q_MAX  42.85013 / Loss  0.06333789229393005\n",
      "fps: 6.900819022109319\n",
      "TIMESTEP 1215 / STATE explore / EPSILON 0.0988871139999983 / ACTION 1 / REWARD 0.1 / Q_MAX  41.604065 / Loss  0.02415090799331665\n",
      "fps: 7.579565209534308\n",
      "TIMESTEP 1216 / STATE explore / EPSILON 0.0988861149999983 / ACTION 1 / REWARD 0.1 / Q_MAX  40.32272 / Loss  0.04562436416745186\n",
      "fps: 6.934762732237365\n",
      "TIMESTEP 1217 / STATE explore / EPSILON 0.0988851159999983 / ACTION 1 / REWARD 0.1 / Q_MAX  39.283936 / Loss  0.5975506901741028\n",
      "fps: 7.046455523523443\n",
      "TIMESTEP 1218 / STATE explore / EPSILON 0.0988841169999983 / ACTION 1 / REWARD 0.1 / Q_MAX  38.470875 / Loss  0.4599010646343231\n",
      "fps: 6.582904209520192\n",
      "TIMESTEP 1219 / STATE explore / EPSILON 0.0988831179999983 / ACTION 1 / REWARD 0.1 / Q_MAX  37.686604 / Loss  0.1585279256105423\n",
      "fps: 4.051031180248163\n",
      "TIMESTEP 1220 / STATE explore / EPSILON 0.0988821189999983 / ACTION 1 / REWARD 0.1 / Q_MAX  36.974213 / Loss  1.2983664274215698\n",
      "fps: 4.3142487435686965\n",
      "TIMESTEP 1221 / STATE explore / EPSILON 0.0988811199999983 / ACTION 1 / REWARD 0.1 / Q_MAX  36.165195 / Loss  37.517330169677734\n",
      "fps: 5.131375216544488\n",
      "TIMESTEP 1222 / STATE explore / EPSILON 0.0988801209999983 / ACTION 1 / REWARD 0.1 / Q_MAX  34.84801 / Loss  0.8705442547798157\n",
      "fps: 6.635202624780701\n",
      "TIMESTEP 1223 / STATE explore / EPSILON 0.09887912199999829 / ACTION 1 / REWARD 0.1 / Q_MAX  33.790546 / Loss  4.030360221862793\n",
      "fps: 7.117482555456002\n",
      "TIMESTEP 1224 / STATE explore / EPSILON 0.09887812299999829 / ACTION 0 / REWARD 0.1 / Q_MAX  22.592203 / Loss  0.637536883354187\n",
      "fps: 9.198317492203708\n",
      "TIMESTEP 1225 / STATE explore / EPSILON 0.09887712399999829 / ACTION 0 / REWARD 0.1 / Q_MAX  32.584084 / Loss  0.3864938020706177\n",
      "fps: 6.614932830336274\n",
      "TIMESTEP 1226 / STATE explore / EPSILON 0.09887612499999829 / ACTION 1 / REWARD 0.1 / Q_MAX  32.216934 / Loss  0.17960411310195923\n",
      "----------Random Action----------\n",
      "fps: 9.905332291073803\n",
      "TIMESTEP 1227 / STATE explore / EPSILON 0.09887512599999829 / ACTION 0 / REWARD 0.1 / Q_MAX  31.810253 / Loss  0.027990277856588364\n",
      "fps: 6.599923211037469\n",
      "TIMESTEP 1228 / STATE explore / EPSILON 0.09887412699999829 / ACTION 1 / REWARD 0.1 / Q_MAX  31.391762 / Loss  0.6693997383117676\n",
      "fps: 10.076284969549915\n",
      "TIMESTEP 1229 / STATE explore / EPSILON 0.09887312799999828 / ACTION 0 / REWARD 0.1 / Q_MAX  31.137033 / Loss  0.43277350068092346\n",
      "fps: 6.669970135202613\n",
      "TIMESTEP 1230 / STATE explore / EPSILON 0.09887212899999828 / ACTION 1 / REWARD 0.1 / Q_MAX  31.009407 / Loss  0.7186233997344971\n",
      "fps: 7.87883182336466\n",
      "TIMESTEP 1231 / STATE explore / EPSILON 0.09887112999999828 / ACTION 1 / REWARD 0.1 / Q_MAX  31.016533 / Loss  0.4249138832092285\n",
      "fps: 5.648491084138102\n",
      "TIMESTEP 1232 / STATE explore / EPSILON 0.09887013099999828 / ACTION 1 / REWARD 0.1 / Q_MAX  31.124338 / Loss  0.4108952283859253\n",
      "fps: 5.545182558663753\n",
      "TIMESTEP 1233 / STATE explore / EPSILON 0.09886913199999828 / ACTION 1 / REWARD 0.1 / Q_MAX  31.045732 / Loss  0.32891950011253357\n",
      "----------Random Action----------\n",
      "fps: 8.28025261428941\n",
      "TIMESTEP 1234 / STATE explore / EPSILON 0.09886813299999828 / ACTION 0 / REWARD 0.1 / Q_MAX  31.081589 / Loss  0.23236019909381866\n",
      "fps: 5.795462898737361\n",
      "TIMESTEP 1235 / STATE explore / EPSILON 0.09886713399999827 / ACTION 1 / REWARD 0.1 / Q_MAX  27.979416 / Loss  0.06574976444244385\n",
      "fps: 6.582924873145842\n",
      "TIMESTEP 1236 / STATE explore / EPSILON 0.09886613499999827 / ACTION 1 / REWARD 0.1 / Q_MAX  31.283508 / Loss  0.11419221758842468\n",
      "----------Random Action----------\n",
      "fps: 8.070268200560683\n",
      "TIMESTEP 1237 / STATE explore / EPSILON 0.09886513599999827 / ACTION 1 / REWARD 0.1 / Q_MAX  31.292196 / Loss  0.012175563722848892\n",
      "fps: 5.6836083012066965\n",
      "TIMESTEP 1238 / STATE explore / EPSILON 0.09886413699999827 / ACTION 1 / REWARD 0.1 / Q_MAX  31.403435 / Loss  0.340969443321228\n",
      "fps: 9.958223129703935\n",
      "TIMESTEP 1239 / STATE explore / EPSILON 0.09886313799999827 / ACTION 0 / REWARD 0.1 / Q_MAX  31.392626 / Loss  0.02569108083844185\n",
      "fps: 7.877929122559221\n",
      "TIMESTEP 1240 / STATE explore / EPSILON 0.09886213899999827 / ACTION 0 / REWARD 0.1 / Q_MAX  31.331991 / Loss  0.29832035303115845\n",
      "fps: 8.045607654474445\n",
      "TIMESTEP 1241 / STATE explore / EPSILON 0.09886113999999827 / ACTION 0 / REWARD 0.1 / Q_MAX  23.646702 / Loss  0.09266416728496552\n",
      "fps: 7.137965129636405\n",
      "TIMESTEP 1242 / STATE explore / EPSILON 0.09886014099999826 / ACTION 0 / REWARD 0.1 / Q_MAX  28.166975 / Loss  0.05998658016324043\n",
      "fps: 7.415802966813416\n",
      "TIMESTEP 1243 / STATE explore / EPSILON 0.09885914199999826 / ACTION 1 / REWARD 0.1 / Q_MAX  31.07383 / Loss  0.2752435505390167\n",
      "fps: 5.697095153881052\n",
      "TIMESTEP 1244 / STATE explore / EPSILON 0.09885814299999826 / ACTION 1 / REWARD 0.1 / Q_MAX  30.893557 / Loss  0.03896713629364967\n",
      "fps: 7.972567535716187\n",
      "TIMESTEP 1245 / STATE explore / EPSILON 0.09885714399999826 / ACTION 0 / REWARD 0.1 / Q_MAX  30.717045 / Loss  0.12814675271511078\n",
      "fps: 5.054224743661891\n",
      "TIMESTEP 1246 / STATE explore / EPSILON 0.09885614499999826 / ACTION 1 / REWARD -1 / Q_MAX  23.056889 / Loss  0.35165491700172424\n",
      "fps: 7.63824234819775\n",
      "TIMESTEP 1247 / STATE explore / EPSILON 0.09885514599999826 / ACTION 0 / REWARD 0.1 / Q_MAX  30.347782 / Loss  0.014613800682127476\n",
      "----------Random Action----------\n",
      "fps: 8.068839875725018\n",
      "TIMESTEP 1248 / STATE explore / EPSILON 0.09885414699999825 / ACTION 0 / REWARD 0.1 / Q_MAX  30.165394 / Loss  0.04868403822183609\n",
      "fps: 6.582904209520192\n",
      "TIMESTEP 1249 / STATE explore / EPSILON 0.09885314799999825 / ACTION 1 / REWARD 0.1 / Q_MAX  29.912754 / Loss  0.03407781571149826\n",
      "fps: 4.002458174767065\n",
      "TIMESTEP 1250 / STATE explore / EPSILON 0.09885214899999825 / ACTION 1 / REWARD 0.1 / Q_MAX  29.57992 / Loss  0.0072833094745874405\n",
      "fps: 4.810643666559618\n",
      "TIMESTEP 1251 / STATE explore / EPSILON 0.09885114999999825 / ACTION 1 / REWARD 0.1 / Q_MAX  29.228907 / Loss  0.0182932298630476\n",
      "fps: 6.806934975275122\n",
      "TIMESTEP 1252 / STATE explore / EPSILON 0.09885015099999825 / ACTION 0 / REWARD 0.1 / Q_MAX  28.909252 / Loss  0.031898025423288345\n",
      "fps: 6.293085909250496\n",
      "TIMESTEP 1253 / STATE explore / EPSILON 0.09884915199999825 / ACTION 0 / REWARD 0.1 / Q_MAX  28.456976 / Loss  0.5297834277153015\n",
      "fps: 8.454434224874875\n",
      "TIMESTEP 1254 / STATE explore / EPSILON 0.09884815299999825 / ACTION 0 / REWARD 0.1 / Q_MAX  28.182234 / Loss  0.03802487254142761\n",
      "fps: 6.107646382129803\n",
      "TIMESTEP 1255 / STATE explore / EPSILON 0.09884715399999824 / ACTION 1 / REWARD 0.1 / Q_MAX  20.760788 / Loss  0.1592859923839569\n",
      "fps: 9.179977106437557\n",
      "TIMESTEP 1256 / STATE explore / EPSILON 0.09884615499999824 / ACTION 0 / REWARD 0.1 / Q_MAX  27.451788 / Loss  0.06855331361293793\n",
      "fps: 6.332994108356711\n",
      "TIMESTEP 1257 / STATE explore / EPSILON 0.09884515599999824 / ACTION 0 / REWARD 0.1 / Q_MAX  18.814735 / Loss  0.35513290762901306\n",
      "fps: 6.101311672385929\n",
      "TIMESTEP 1258 / STATE explore / EPSILON 0.09884415699999824 / ACTION 0 / REWARD 0.1 / Q_MAX  21.325518 / Loss  0.13326896727085114\n",
      "fps: 6.497568623076935\n",
      "TIMESTEP 1259 / STATE explore / EPSILON 0.09884315799999824 / ACTION 1 / REWARD 0.1 / Q_MAX  17.134356 / Loss  0.16286763548851013\n",
      "----------Random Action----------\n",
      "fps: 7.212309216092223\n",
      "TIMESTEP 1260 / STATE explore / EPSILON 0.09884215899999824 / ACTION 0 / REWARD 0.1 / Q_MAX  25.990036 / Loss  0.036330047994852066\n",
      "fps: 6.604027944633475\n",
      "TIMESTEP 1261 / STATE explore / EPSILON 0.09884115999999823 / ACTION 1 / REWARD 0.1 / Q_MAX  18.603624 / Loss  0.402167409658432\n",
      "fps: 6.62678455189499\n",
      "TIMESTEP 1262 / STATE explore / EPSILON 0.09884016099999823 / ACTION 1 / REWARD 0.1 / Q_MAX  25.30091 / Loss  0.03643227741122246\n",
      "fps: 8.069476349336442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 1263 / STATE explore / EPSILON 0.09883916199999823 / ACTION 0 / REWARD 0.1 / Q_MAX  25.099518 / Loss  0.27567601203918457\n",
      "fps: 8.701045128473217\n",
      "TIMESTEP 1264 / STATE explore / EPSILON 0.09883816299999823 / ACTION 0 / REWARD 0.1 / Q_MAX  16.105158 / Loss  3.9388439655303955\n",
      "fps: 6.215017499770325\n",
      "TIMESTEP 1265 / STATE explore / EPSILON 0.09883716399999823 / ACTION 1 / REWARD 0.1 / Q_MAX  24.355135 / Loss  0.13831022381782532\n",
      "fps: 6.253789426285483\n",
      "TIMESTEP 1266 / STATE explore / EPSILON 0.09883616499999823 / ACTION 1 / REWARD 0.1 / Q_MAX  23.948141 / Loss  11.696100234985352\n",
      "----------Random Action----------\n",
      "fps: 8.47983405476112\n",
      "TIMESTEP 1267 / STATE explore / EPSILON 0.09883516599999823 / ACTION 0 / REWARD 0.1 / Q_MAX  23.297754 / Loss  3.1381173133850098\n",
      "fps: 8.408553087924558\n",
      "TIMESTEP 1268 / STATE explore / EPSILON 0.09883416699999822 / ACTION 0 / REWARD 0.1 / Q_MAX  22.764547 / Loss  0.010542858392000198\n",
      "fps: 7.734116154538347\n",
      "TIMESTEP 1269 / STATE explore / EPSILON 0.09883316799999822 / ACTION 0 / REWARD 0.1 / Q_MAX  22.289629 / Loss  0.10661020874977112\n",
      "fps: 9.351639874250296\n",
      "TIMESTEP 1270 / STATE explore / EPSILON 0.09883216899999822 / ACTION 0 / REWARD 0.1 / Q_MAX  21.879221 / Loss  0.016660572960972786\n",
      "fps: 8.483006900792013\n",
      "TIMESTEP 1271 / STATE explore / EPSILON 0.09883116999999822 / ACTION 0 / REWARD 0.1 / Q_MAX  21.496256 / Loss  6.172203063964844\n",
      "fps: 8.625587387895028\n",
      "TIMESTEP 1272 / STATE explore / EPSILON 0.09883017099999822 / ACTION 0 / REWARD 0.1 / Q_MAX  21.109594 / Loss  0.026387833058834076\n",
      "fps: 6.138754482253933\n",
      "TIMESTEP 1273 / STATE explore / EPSILON 0.09882917199999822 / ACTION 1 / REWARD 0.1 / Q_MAX  20.75331 / Loss  0.06252880394458771\n",
      "----------Random Action----------\n",
      "fps: 7.295215666357648\n",
      "TIMESTEP 1274 / STATE explore / EPSILON 0.09882817299999822 / ACTION 1 / REWARD 0.1 / Q_MAX  20.443464 / Loss  0.0696449875831604\n",
      "fps: 6.064357924445369\n",
      "TIMESTEP 1275 / STATE explore / EPSILON 0.09882717399999821 / ACTION 1 / REWARD 0.1 / Q_MAX  20.124994 / Loss  0.028417889028787613\n",
      "fps: 6.715517420813393\n",
      "TIMESTEP 1276 / STATE explore / EPSILON 0.09882617499999821 / ACTION 0 / REWARD 0.1 / Q_MAX  19.884567 / Loss  0.2624262571334839\n",
      "fps: 7.697072241531815\n",
      "TIMESTEP 1277 / STATE explore / EPSILON 0.09882517599999821 / ACTION 0 / REWARD 0.1 / Q_MAX  19.697708 / Loss  0.034783653914928436\n",
      "fps: 5.266420901429389\n",
      "TIMESTEP 1278 / STATE explore / EPSILON 0.09882417699999821 / ACTION 1 / REWARD 0.1 / Q_MAX  19.531208 / Loss  1.932196021080017\n",
      "fps: 7.756758006321096\n",
      "TIMESTEP 1279 / STATE explore / EPSILON 0.09882317799999821 / ACTION 0 / REWARD -1 / Q_MAX  19.328854 / Loss  0.41531679034233093\n",
      "fps: 5.408745955662845\n",
      "TIMESTEP 1280 / STATE explore / EPSILON 0.0988221789999982 / ACTION 1 / REWARD 0.1 / Q_MAX  19.166477 / Loss  0.011355457827448845\n",
      "fps: 4.169226104978554\n",
      "TIMESTEP 1281 / STATE explore / EPSILON 0.0988211799999982 / ACTION 1 / REWARD 0.1 / Q_MAX  19.020329 / Loss  0.03763047605752945\n",
      "fps: 5.438129232428816\n",
      "TIMESTEP 1282 / STATE explore / EPSILON 0.0988201809999982 / ACTION 0 / REWARD 0.1 / Q_MAX  18.88337 / Loss  0.17202594876289368\n",
      "fps: 9.10443510575487\n",
      "TIMESTEP 1283 / STATE explore / EPSILON 0.0988191819999982 / ACTION 0 / REWARD 0.1 / Q_MAX  18.807325 / Loss  0.049111925065517426\n",
      "fps: 5.956092268717588\n",
      "TIMESTEP 1284 / STATE explore / EPSILON 0.0988181829999982 / ACTION 1 / REWARD 0.1 / Q_MAX  18.714151 / Loss  2.857687473297119\n",
      "----------Random Action----------\n",
      "fps: 8.854944813666956\n",
      "TIMESTEP 1285 / STATE explore / EPSILON 0.0988171839999982 / ACTION 0 / REWARD 0.1 / Q_MAX  18.541536 / Loss  0.015184955671429634\n",
      "fps: 6.027441986123805\n",
      "TIMESTEP 1286 / STATE explore / EPSILON 0.0988161849999982 / ACTION 1 / REWARD 0.1 / Q_MAX  18.390226 / Loss  0.482296347618103\n",
      "fps: 7.216019296377284\n",
      "TIMESTEP 1287 / STATE explore / EPSILON 0.0988151859999982 / ACTION 0 / REWARD 0.1 / Q_MAX  18.289122 / Loss  0.0034697353839874268\n",
      "fps: 8.338361036394685\n",
      "TIMESTEP 1288 / STATE explore / EPSILON 0.0988141869999982 / ACTION 0 / REWARD 0.1 / Q_MAX  18.176947 / Loss  0.05668461322784424\n",
      "fps: 5.590050058908513\n",
      "TIMESTEP 1289 / STATE explore / EPSILON 0.09881318799999819 / ACTION 1 / REWARD 0.1 / Q_MAX  18.100416 / Loss  0.01296122558414936\n",
      "fps: 6.670797574893918\n",
      "TIMESTEP 1290 / STATE explore / EPSILON 0.09881218899999819 / ACTION 1 / REWARD 0.1 / Q_MAX  18.016695 / Loss  0.049506768584251404\n",
      "fps: 5.621449489026638\n",
      "TIMESTEP 1291 / STATE explore / EPSILON 0.09881118999999819 / ACTION 1 / REWARD 0.1 / Q_MAX  17.928423 / Loss  0.03978227823972702\n",
      "fps: 6.414201364410032\n",
      "TIMESTEP 1292 / STATE explore / EPSILON 0.09881019099999819 / ACTION 0 / REWARD 0.1 / Q_MAX  17.853218 / Loss  4.0035719871521\n",
      "fps: 6.8535050997232\n",
      "TIMESTEP 1293 / STATE explore / EPSILON 0.09880919199999819 / ACTION 0 / REWARD 0.1 / Q_MAX  17.697386 / Loss  0.05668048560619354\n",
      "fps: 6.853594690074609\n",
      "TIMESTEP 1294 / STATE explore / EPSILON 0.09880819299999818 / ACTION 0 / REWARD 0.1 / Q_MAX  17.557133 / Loss  0.06896905601024628\n",
      "fps: 5.68531860010871\n",
      "TIMESTEP 1295 / STATE explore / EPSILON 0.09880719399999818 / ACTION 1 / REWARD 0.1 / Q_MAX  17.385813 / Loss  0.18820622563362122\n",
      "----------Random Action----------\n",
      "fps: 6.4555761114822126\n",
      "TIMESTEP 1296 / STATE explore / EPSILON 0.09880619499999818 / ACTION 1 / REWARD 0.1 / Q_MAX  17.354025 / Loss  0.08227266371250153\n",
      "fps: 7.198692864302289\n",
      "TIMESTEP 1297 / STATE explore / EPSILON 0.09880519599999818 / ACTION 0 / REWARD 0.1 / Q_MAX  17.236975 / Loss  0.1337619423866272\n",
      "fps: 6.9972240017083065\n",
      "TIMESTEP 1298 / STATE explore / EPSILON 0.09880419699999818 / ACTION 0 / REWARD 0.1 / Q_MAX  17.152441 / Loss  0.0878901481628418\n",
      "fps: 6.8068245073361995\n",
      "TIMESTEP 1299 / STATE explore / EPSILON 0.09880319799999818 / ACTION 0 / REWARD 0.1 / Q_MAX  17.073282 / Loss  0.18998153507709503\n",
      "fps: 5.35090132040569\n",
      "TIMESTEP 1300 / STATE explore / EPSILON 0.09880219899999818 / ACTION 1 / REWARD 0.1 / Q_MAX  10.640331 / Loss  0.4795096516609192\n",
      "----------Random Action----------\n",
      "fps: 5.308002556363383\n",
      "TIMESTEP 1301 / STATE explore / EPSILON 0.09880119999999817 / ACTION 1 / REWARD 0.1 / Q_MAX  9.39693 / Loss  0.0034361646976321936\n",
      "fps: 7.756729316425358\n",
      "TIMESTEP 1302 / STATE explore / EPSILON 0.09880020099999817 / ACTION 0 / REWARD 0.1 / Q_MAX  16.917406 / Loss  0.14116117358207703\n",
      "fps: 6.151809914931065\n",
      "TIMESTEP 1303 / STATE explore / EPSILON 0.09879920199999817 / ACTION 0 / REWARD 0.1 / Q_MAX  16.890984 / Loss  0.18931987881660461\n",
      "fps: 7.4666153974874545\n",
      "TIMESTEP 1304 / STATE explore / EPSILON 0.09879820299999817 / ACTION 0 / REWARD 0.1 / Q_MAX  16.868853 / Loss  0.021776821464300156\n",
      "fps: 6.9007395464005725\n",
      "TIMESTEP 1305 / STATE explore / EPSILON 0.09879720399999817 / ACTION 1 / REWARD 0.1 / Q_MAX  16.836103 / Loss  0.010106412693858147\n",
      "fps: 5.991734462832939\n",
      "TIMESTEP 1306 / STATE explore / EPSILON 0.09879620499999817 / ACTION 1 / REWARD 0.1 / Q_MAX  16.788647 / Loss  0.00709132943302393\n",
      "fps: 8.067955188969593\n",
      "TIMESTEP 1307 / STATE explore / EPSILON 0.09879520599999816 / ACTION 0 / REWARD 0.1 / Q_MAX  16.735762 / Loss  0.005539636127650738\n",
      "fps: 9.011346082368132\n",
      "TIMESTEP 1308 / STATE explore / EPSILON 0.09879420699999816 / ACTION 0 / REWARD 0.1 / Q_MAX  16.676723 / Loss  0.04144642874598503\n",
      "fps: 8.625942172946448\n",
      "TIMESTEP 1309 / STATE explore / EPSILON 0.09879320799999816 / ACTION 0 / REWARD 0.1 / Q_MAX  16.60064 / Loss  0.017919056117534637\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-72cb52f7ef17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplay_Game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-e511e924bf20>\u001b[0m in \u001b[0;36mplay_Game\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtrain_Network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-24f15ce9f55b>\u001b[0m in \u001b[0;36mtrain_Network\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#run the selected action and observed next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fps: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# helpful for measuring frame rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-999ca9103e92>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mactions_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# storing actions in a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mgame_over\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;31m#game over\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-05fcd6689114>\u001b[0m in \u001b[0;36mget_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return Runner.instance_.distanceMeter.digits\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_array\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the javascript object is of type array with score in the formate[1,0,0] which is 100.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "play_Game(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}