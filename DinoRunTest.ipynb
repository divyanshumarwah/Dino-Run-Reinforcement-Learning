{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 3 of /Users/divyanshumarwah/anaconda3/lib/python3.7/site-packages/googleapis_common_protos-1.6.0-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/Users/divyanshumarwah/anaconda3/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n",
      "Requirement already satisfied: selenium in /Users/divyanshumarwah/anaconda3/lib/python3.7/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /Users/divyanshumarwah/anaconda3/lib/python3.7/site-packages (from selenium) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "!pip install selenium\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"chrome://dino\"\n",
    "chrome_driver_path = \"./chrome_driver/chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.get('chrome://dino')\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('./objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('./objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[:300, :500] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call only once to init file structure\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "#init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)    \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 274000 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.371055 / Loss  0.09538985788822174\n",
      "fps: 0.1654629524175427\n",
      "TIMESTEP 274001 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD -1 / Q_MAX  11.764407 / Loss  0.04827740788459778\n",
      "fps: 5.901029158313109\n",
      "TIMESTEP 274002 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  10.932227 / Loss  0.03227732330560684\n",
      "fps: 5.709588936669539\n",
      "TIMESTEP 274003 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.016386 / Loss  0.035667479038238525\n",
      "fps: 8.172271625528996\n",
      "TIMESTEP 274004 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  3.2347562 / Loss  0.3056308627128601\n",
      "fps: 7.9687504749745415\n",
      "TIMESTEP 274005 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.075692 / Loss  0.042980749160051346\n",
      "fps: 8.035357608801853\n",
      "TIMESTEP 274006 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.25996 / Loss  0.01746058091521263\n",
      "fps: 7.8197085626500815\n",
      "TIMESTEP 274007 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.189562 / Loss  0.021447371691465378\n",
      "fps: 7.473333855989281\n",
      "TIMESTEP 274008 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.665297 / Loss  0.033553242683410645\n",
      "fps: 8.332629395719556\n",
      "TIMESTEP 274009 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.422248 / Loss  0.028796061873435974\n",
      "fps: 8.44881425034798\n",
      "TIMESTEP 274010 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.44791 / Loss  0.05285874754190445\n",
      "fps: 8.637986675316384\n",
      "TIMESTEP 274011 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.618636 / Loss  0.05461733788251877\n",
      "fps: 7.588629821479295\n",
      "TIMESTEP 274012 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.264126 / Loss  0.037069521844387054\n",
      "fps: 7.956898514212867\n",
      "TIMESTEP 274013 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.297192 / Loss  0.1431514024734497\n",
      "fps: 8.33474885291248\n",
      "TIMESTEP 274014 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.159003 / Loss  0.05350419506430626\n",
      "fps: 7.391599493868977\n",
      "TIMESTEP 274015 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.385948 / Loss  0.02552879974246025\n",
      "fps: 8.211743469161206\n",
      "TIMESTEP 274016 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.496061 / Loss  0.22263570129871368\n",
      "fps: 7.809560339097292\n",
      "TIMESTEP 274017 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.779486 / Loss  0.022175125777721405\n",
      "fps: 7.21642900401916\n",
      "TIMESTEP 274018 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.742084 / Loss  0.03929836302995682\n",
      "fps: 8.630059813461012\n",
      "TIMESTEP 274019 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.580003 / Loss  0.03890562057495117\n",
      "fps: 8.537223995310363\n",
      "TIMESTEP 274020 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.713762 / Loss  0.046968914568424225\n",
      "fps: 7.67017352646969\n",
      "TIMESTEP 274021 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  10.987249 / Loss  0.0526224747300148\n",
      "fps: 8.39292196188841\n",
      "TIMESTEP 274022 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.667477 / Loss  0.09137166291475296\n",
      "fps: 8.344465776044723\n",
      "TIMESTEP 274023 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.001019 / Loss  0.026566922664642334\n",
      "fps: 8.94031482803824\n",
      "TIMESTEP 274024 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.82706 / Loss  0.02087266743183136\n",
      "fps: 8.964429141473403\n",
      "TIMESTEP 274025 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.295079 / Loss  0.03557245433330536\n",
      "fps: 9.206494564084103\n",
      "TIMESTEP 274026 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.915766 / Loss  0.020872153341770172\n",
      "fps: 9.030515072374525\n",
      "TIMESTEP 274027 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.695552 / Loss  0.03541294485330582\n",
      "fps: 8.678379077394192\n",
      "TIMESTEP 274028 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.868191 / Loss  0.01625949889421463\n",
      "fps: 8.761021502007328\n",
      "TIMESTEP 274029 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.870563 / Loss  0.024129830300807953\n",
      "fps: 8.852758107580442\n",
      "TIMESTEP 274030 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.856315 / Loss  0.04891648888587952\n",
      "fps: 8.873021749385979\n",
      "TIMESTEP 274031 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.803316 / Loss  0.03395947068929672\n",
      "fps: 8.938066713122492\n",
      "TIMESTEP 274032 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.805078 / Loss  0.04049759358167648\n",
      "fps: 9.04404826592836\n",
      "TIMESTEP 274033 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.61386 / Loss  0.02701600082218647\n",
      "fps: 8.795985261396309\n",
      "TIMESTEP 274034 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.586514 / Loss  0.02424173429608345\n",
      "fps: 9.011133120496371\n",
      "TIMESTEP 274035 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.725213 / Loss  0.03852983936667442\n",
      "fps: 8.820977009076914\n",
      "TIMESTEP 274036 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.626461 / Loss  0.03266790881752968\n",
      "fps: 9.08040796090988\n",
      "TIMESTEP 274037 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.100518 / Loss  0.5870848894119263\n",
      "fps: 7.449230266476276\n",
      "TIMESTEP 274038 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.547461 / Loss  0.03938021510839462\n",
      "fps: 7.479037388910287\n",
      "TIMESTEP 274039 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.616401 / Loss  0.05283087491989136\n",
      "fps: 5.245246917671599\n",
      "TIMESTEP 274040 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.040489 / Loss  0.03219419717788696\n",
      "fps: 8.438666167706836\n",
      "TIMESTEP 274041 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.608184 / Loss  0.03206314891576767\n",
      "fps: 8.499147917819156\n",
      "TIMESTEP 274042 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.885311 / Loss  0.050893865525722504\n",
      "fps: 8.56602756288242\n",
      "TIMESTEP 274043 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.434442 / Loss  0.037986598908901215\n",
      "fps: 8.933916388522645\n",
      "TIMESTEP 274044 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.828567 / Loss  0.029702482745051384\n",
      "fps: 8.46518405496118\n",
      "TIMESTEP 274045 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.788142 / Loss  0.02690364234149456\n",
      "fps: 6.719078300263681\n",
      "TIMESTEP 274046 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.301604 / Loss  0.04230258986353874\n",
      "fps: 5.673659705191279\n",
      "TIMESTEP 274047 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.787501 / Loss  0.1384955197572708\n",
      "fps: 7.569497532963007\n",
      "TIMESTEP 274048 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.667716 / Loss  0.023684313520789146\n",
      "fps: 7.2974114683192814\n",
      "TIMESTEP 274049 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.409944 / Loss  0.3422132730484009\n",
      "fps: 8.211856011214616\n",
      "TIMESTEP 274050 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.929032 / Loss  0.2724064290523529\n",
      "fps: 8.497770358019114\n",
      "TIMESTEP 274051 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.78307 / Loss  0.05492257699370384\n",
      "fps: 8.037082102979866\n",
      "TIMESTEP 274052 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.987638 / Loss  0.02277986891567707\n",
      "fps: 7.854045926172825\n",
      "TIMESTEP 274053 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.11709 / Loss  0.04266299307346344\n",
      "fps: 8.22801362997024\n",
      "TIMESTEP 274054 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.033762 / Loss  0.04559299349784851\n",
      "fps: 8.327649604892189\n",
      "TIMESTEP 274055 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.9857855 / Loss  0.1684086173772812\n",
      "fps: 8.102085836478764\n",
      "TIMESTEP 274056 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.180447 / Loss  0.053221553564071655\n",
      "fps: 8.366804108493266\n",
      "TIMESTEP 274057 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.87255 / Loss  0.08478301763534546\n",
      "fps: 8.19624219566768\n",
      "TIMESTEP 274058 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.5831 / Loss  0.030452176928520203\n",
      "fps: 6.865115335071584\n",
      "TIMESTEP 274059 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.06642 / Loss  0.022938497364521027\n",
      "fps: 7.9214933784590285\n",
      "TIMESTEP 274060 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.886581 / Loss  0.3063327372074127\n",
      "fps: 8.244980440722612\n",
      "TIMESTEP 274061 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.745284 / Loss  0.2625708281993866\n",
      "fps: 7.71373345961802\n",
      "TIMESTEP 274062 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.494742 / Loss  0.0212081465870142\n",
      "fps: 7.752070026023184\n",
      "TIMESTEP 274063 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.806786 / Loss  0.02351095899939537\n",
      "fps: 7.891595326346685\n",
      "TIMESTEP 274064 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.1721325 / Loss  3.331547975540161\n",
      "fps: 8.206553810272906\n",
      "TIMESTEP 274065 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.000859 / Loss  0.04062764719128609\n",
      "fps: 7.241922060880225\n",
      "TIMESTEP 274066 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.078707 / Loss  0.029313083738088608\n",
      "fps: 8.259056440671625\n",
      "TIMESTEP 274067 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.257983 / Loss  0.022369980812072754\n",
      "fps: 7.867806421720625\n",
      "TIMESTEP 274068 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.76297 / Loss  0.06202514469623566\n",
      "fps: 7.705287678128232\n",
      "TIMESTEP 274069 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.642493 / Loss  0.05264494568109512\n",
      "fps: 8.007743694406631\n",
      "TIMESTEP 274070 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.771094 / Loss  0.047464072704315186\n",
      "fps: 8.315234371902333\n",
      "TIMESTEP 274071 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.3892975 / Loss  0.12986883521080017\n",
      "fps: 6.629675526668521\n",
      "TIMESTEP 274072 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.33731 / Loss  0.04764964058995247\n",
      "fps: 6.845418031489252\n",
      "TIMESTEP 274073 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.38225 / Loss  0.02822824940085411\n",
      "fps: 6.080578497420218\n",
      "TIMESTEP 274074 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.657605 / Loss  0.018422404304146767\n",
      "fps: 5.665245280317468\n",
      "TIMESTEP 274075 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.550786 / Loss  0.029497768729925156\n",
      "fps: 5.414506313884349\n",
      "TIMESTEP 274076 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.655377 / Loss  0.029427744448184967\n",
      "fps: 8.287615122121823\n",
      "TIMESTEP 274077 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.927041 / Loss  0.03273113816976547\n",
      "fps: 6.882779693169082\n",
      "TIMESTEP 274078 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.078611 / Loss  0.047550540417432785\n",
      "fps: 6.285871003613289\n",
      "TIMESTEP 274079 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  10.982066 / Loss  0.057708632200956345\n",
      "fps: 5.568061767286756\n",
      "TIMESTEP 274080 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.319352 / Loss  0.026114415377378464\n",
      "fps: 5.762198104135184\n",
      "TIMESTEP 274081 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.303447 / Loss  0.00963598396629095\n",
      "fps: 6.277300686950926\n",
      "TIMESTEP 274082 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.360209 / Loss  0.02815275639295578\n",
      "fps: 6.59682985061481\n",
      "TIMESTEP 274083 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.28843 / Loss  0.01993160881102085\n",
      "fps: 6.699491904169388\n",
      "TIMESTEP 274084 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.613816 / Loss  0.025397902354598045\n",
      "fps: 7.762169844841881\n",
      "TIMESTEP 274085 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.2271595 / Loss  0.03477386385202408\n",
      "fps: 6.807542613036944\n",
      "TIMESTEP 274086 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.481185 / Loss  0.03652777895331383\n",
      "fps: 6.5033111145222335\n",
      "TIMESTEP 274087 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.330896 / Loss  0.10671047866344452\n",
      "fps: 6.636588464818662\n",
      "TIMESTEP 274088 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.105916 / Loss  0.016854342073202133\n",
      "fps: 5.665620255353177\n",
      "TIMESTEP 274089 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.336998 / Loss  0.019860152155160904\n",
      "fps: 5.907378899763525\n",
      "TIMESTEP 274090 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.057453 / Loss  0.019239993765950203\n",
      "fps: 7.694276489580276\n",
      "TIMESTEP 274091 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.343565 / Loss  0.015024082735180855\n",
      "fps: 8.052172143500272\n",
      "TIMESTEP 274092 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  10.993069 / Loss  0.034049615263938904\n",
      "fps: 6.50351278977996\n",
      "TIMESTEP 274093 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.3300085 / Loss  0.03708644583821297\n",
      "fps: 7.239234705765596\n",
      "TIMESTEP 274094 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.33427 / Loss  0.02518070861697197\n",
      "fps: 7.643810402045491\n",
      "TIMESTEP 274095 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.257497 / Loss  0.3966214060783386\n",
      "fps: 6.5260579958643286\n",
      "TIMESTEP 274096 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.088698 / Loss  0.028040733188390732\n",
      "fps: 6.043360925601735\n",
      "TIMESTEP 274097 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.365993 / Loss  0.038427434861660004\n",
      "fps: 5.5713456467884725\n",
      "TIMESTEP 274098 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.565205 / Loss  0.06197768449783325\n",
      "fps: 4.915432330981269\n",
      "TIMESTEP 274099 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.635271 / Loss  0.0383235439658165\n",
      "fps: 6.198704780371304\n",
      "TIMESTEP 274100 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.545639 / Loss  0.018296366557478905\n",
      "fps: 7.643810402045491\n",
      "TIMESTEP 274101 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.055357 / Loss  0.022855862975120544\n",
      "fps: 6.970003240467625\n",
      "TIMESTEP 274102 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.618053 / Loss  0.032608963549137115\n",
      "fps: 6.775749292830407\n",
      "TIMESTEP 274103 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.395096 / Loss  0.0544717013835907\n",
      "fps: 6.741725750272044\n",
      "TIMESTEP 274104 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.435964 / Loss  0.05176408961415291\n",
      "fps: 6.3323630654025\n",
      "TIMESTEP 274105 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  10.923321 / Loss  0.02392999827861786\n",
      "fps: 6.891227055553\n",
      "TIMESTEP 274106 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.33775 / Loss  0.028950760141015053\n",
      "fps: 8.130481474158419\n",
      "TIMESTEP 274107 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.354283 / Loss  0.017619682475924492\n",
      "fps: 7.0667450680423975\n",
      "TIMESTEP 274108 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.27739 / Loss  0.026493282988667488\n",
      "fps: 7.778883549614144\n",
      "TIMESTEP 274109 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.982434 / Loss  0.028929058462381363\n",
      "fps: 8.00545109947684\n",
      "TIMESTEP 274110 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.352943 / Loss  0.04428160935640335\n",
      "fps: 7.352288264536983\n",
      "TIMESTEP 274111 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.331455 / Loss  0.048815492540597916\n",
      "fps: 5.34205525582438\n",
      "TIMESTEP 274112 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.503528 / Loss  0.02419476956129074\n",
      "fps: 7.883007655022187\n",
      "TIMESTEP 274113 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.396849 / Loss  0.012672249227762222\n",
      "fps: 7.829634718198381\n",
      "TIMESTEP 274114 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.204272 / Loss  0.08658692240715027\n",
      "fps: 6.948744710953834\n",
      "TIMESTEP 274115 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.130908 / Loss  0.038465868681669235\n",
      "fps: 6.853437908496732\n",
      "TIMESTEP 274116 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.495251 / Loss  0.028257329016923904\n",
      "fps: 6.799938393197312\n",
      "TIMESTEP 274117 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.823216 / Loss  0.07884475588798523\n",
      "fps: 7.0019081070342875\n",
      "TIMESTEP 274118 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.219144 / Loss  0.044500891119241714\n",
      "fps: 8.006352696841976\n",
      "TIMESTEP 274119 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.60885 / Loss  0.026369933038949966\n",
      "fps: 7.853795913475942\n",
      "TIMESTEP 274120 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.145443 / Loss  0.024507936090230942\n",
      "fps: 8.25811329373244\n",
      "TIMESTEP 274121 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.776689 / Loss  0.04539690166711807\n",
      "fps: 6.989190369147144\n",
      "TIMESTEP 274122 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.478364 / Loss  0.01897687092423439\n",
      "fps: 8.032833791379788\n",
      "TIMESTEP 274123 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.682401 / Loss  0.02513868734240532\n",
      "fps: 6.915063325782381\n",
      "TIMESTEP 274124 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.12534 / Loss  0.03975003957748413\n",
      "fps: 8.046919695873806\n",
      "TIMESTEP 274125 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.893506 / Loss  0.04255324602127075\n",
      "fps: 7.9373836161870015\n",
      "TIMESTEP 274126 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.784517 / Loss  0.020378505811095238\n",
      "fps: 8.274388343308964\n",
      "TIMESTEP 274127 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.745912 / Loss  0.03491479158401489\n",
      "fps: 7.552596039225996\n",
      "TIMESTEP 274128 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.025818 / Loss  0.03918354958295822\n",
      "fps: 7.930345079250435\n",
      "TIMESTEP 274129 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.449781 / Loss  0.019554175436496735\n",
      "fps: 8.241027676371539\n",
      "TIMESTEP 274130 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.6475525 / Loss  0.04858841747045517\n",
      "fps: 8.238729041281015\n",
      "TIMESTEP 274131 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.076976 / Loss  0.040085095912218094\n",
      "fps: 7.578579146504963\n",
      "TIMESTEP 274132 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.651568 / Loss  0.04431243985891342\n",
      "fps: 7.0655784375658035\n",
      "TIMESTEP 274133 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.441102 / Loss  0.033540915697813034\n",
      "fps: 7.791875041798875\n",
      "TIMESTEP 274134 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.465025 / Loss  0.016215398907661438\n",
      "fps: 7.967297317832991\n",
      "TIMESTEP 274135 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.369937 / Loss  0.015263576991856098\n",
      "fps: 8.267619653983603\n",
      "TIMESTEP 274136 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.517446 / Loss  0.03388933837413788\n",
      "fps: 7.086720711526829\n",
      "TIMESTEP 274137 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.990265 / Loss  0.015269294381141663\n",
      "fps: 6.746497110347257\n",
      "TIMESTEP 274138 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.719452 / Loss  0.015372229740023613\n",
      "fps: 6.947582188048176\n",
      "TIMESTEP 274139 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.332323 / Loss  0.02884567342698574\n",
      "fps: 8.127173095852614\n",
      "TIMESTEP 274140 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.627637 / Loss  0.04978527873754501\n",
      "fps: 8.113919373528564\n",
      "TIMESTEP 274141 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.568735 / Loss  0.03279515355825424\n",
      "fps: 7.875000469386606\n",
      "TIMESTEP 274142 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.326779 / Loss  0.01597578078508377\n",
      "fps: 6.556857938788336\n",
      "TIMESTEP 274143 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.198404 / Loss  0.033856309950351715\n",
      "fps: 8.285699610833449\n",
      "TIMESTEP 274144 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.476935 / Loss  0.02608978934586048\n",
      "fps: 8.485289387278653\n",
      "TIMESTEP 274145 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.430038 / Loss  0.026210356503725052\n",
      "fps: 8.667099232539975\n",
      "TIMESTEP 274146 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.427941 / Loss  0.034748468548059464\n",
      "fps: 7.929070774209891\n",
      "TIMESTEP 274147 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  10.977273 / Loss  0.028163444250822067\n",
      "fps: 5.386338390097253\n",
      "TIMESTEP 274148 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.534629 / Loss  0.006907965987920761\n",
      "fps: 7.539196847931005\n",
      "TIMESTEP 274149 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.28249 / Loss  0.052473947405815125\n",
      "fps: 7.556011133229447\n",
      "TIMESTEP 274150 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.4940605 / Loss  0.017124440521001816\n",
      "fps: 8.01544112307416\n",
      "TIMESTEP 274151 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.184363 / Loss  0.01712617464363575\n",
      "fps: 8.29097349610981\n",
      "TIMESTEP 274152 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.468579 / Loss  0.05067692697048187\n",
      "fps: 8.77203358813749\n",
      "TIMESTEP 274153 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.834282 / Loss  0.05437129735946655\n",
      "fps: 8.285274061899983\n",
      "TIMESTEP 274154 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.299479 / Loss  0.04088892787694931\n",
      "fps: 8.543867153921054\n",
      "TIMESTEP 274155 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.754378 / Loss  0.013626707717776299\n",
      "fps: 8.187985967816559\n",
      "TIMESTEP 274156 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.179754 / Loss  3.5072267055511475\n",
      "fps: 8.645892468064668\n",
      "TIMESTEP 274157 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.648682 / Loss  0.03240830451250076\n",
      "fps: 7.297678100722929\n",
      "TIMESTEP 274158 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.247743 / Loss  0.014641767367720604\n",
      "fps: 8.466328899295533\n",
      "TIMESTEP 274159 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.399679 / Loss  0.03865322098135948\n",
      "fps: 8.382807097488538\n",
      "TIMESTEP 274160 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.885622 / Loss  0.054077811539173126\n",
      "fps: 8.659261889622027\n",
      "TIMESTEP 274161 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.014003 / Loss  0.022239692509174347\n",
      "fps: 8.698158048634816\n",
      "TIMESTEP 274162 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.336825 / Loss  0.0457787811756134\n",
      "fps: 9.174836433316635\n",
      "TIMESTEP 274163 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.56416 / Loss  0.019617324694991112\n",
      "fps: 8.961230637752378\n",
      "TIMESTEP 274164 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.850676 / Loss  0.10153517127037048\n",
      "fps: 7.4558379388253195\n",
      "TIMESTEP 274165 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.296451 / Loss  0.03740920498967171\n",
      "fps: 8.935438995656167\n",
      "TIMESTEP 274166 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.296362 / Loss  0.034235987812280655\n",
      "fps: 8.874842892388163\n",
      "TIMESTEP 274167 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.328644 / Loss  0.0565229170024395\n",
      "fps: 7.705641575849323\n",
      "TIMESTEP 274168 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.890938 / Loss  0.060070935636758804\n",
      "fps: 8.587810860725385\n",
      "TIMESTEP 274169 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.226941 / Loss  0.05553952604532242\n",
      "fps: 8.627077681265542\n",
      "TIMESTEP 274170 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.031687 / Loss  0.015957918018102646\n",
      "fps: 8.679851745467413\n",
      "TIMESTEP 274171 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.755856 / Loss  0.09218361228704453\n",
      "fps: 8.576168811916617\n",
      "TIMESTEP 274172 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.347148 / Loss  0.022895019501447678\n",
      "fps: 8.292481553828026\n",
      "TIMESTEP 274173 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.559598 / Loss  0.024136992171406746\n",
      "fps: 7.495744855761155\n",
      "TIMESTEP 274174 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.293075 / Loss  0.0321924090385437\n",
      "fps: 7.710216859408117\n",
      "TIMESTEP 274175 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.323698 / Loss  0.034529704600572586\n",
      "fps: 8.698140010410468\n",
      "TIMESTEP 274176 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.203442 / Loss  0.046624451875686646\n",
      "fps: 7.473333855989281\n",
      "TIMESTEP 274177 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.776113 / Loss  0.10635694861412048\n",
      "fps: 7.852149168788378\n",
      "TIMESTEP 274178 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.402496 / Loss  0.03711767867207527\n",
      "fps: 8.883207211099604\n",
      "TIMESTEP 274179 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.8195095 / Loss  0.08528763055801392\n",
      "fps: 8.782411323757277\n",
      "TIMESTEP 274180 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.705955 / Loss  0.027921464294195175\n",
      "fps: 8.821143973601579\n",
      "TIMESTEP 274181 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.5299425 / Loss  0.06269049644470215\n",
      "fps: 8.272593153613721\n",
      "TIMESTEP 274182 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.493742 / Loss  0.023385360836982727\n",
      "fps: 8.649779440424167\n",
      "TIMESTEP 274183 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.563084 / Loss  0.12709081172943115\n",
      "fps: 6.11120597963079\n",
      "TIMESTEP 274184 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.43761 / Loss  0.03789324685931206\n",
      "fps: 9.06008731096485\n",
      "TIMESTEP 274185 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.612057 / Loss  0.032193128019571304\n",
      "fps: 8.144453570852969\n",
      "TIMESTEP 274186 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.412721 / Loss  0.024654801934957504\n",
      "fps: 8.159743513921528\n",
      "TIMESTEP 274187 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.398164 / Loss  0.04226251691579819\n",
      "fps: 6.979235201815734\n",
      "TIMESTEP 274188 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.8632765 / Loss  0.0470927469432354\n",
      "fps: 8.971101628755067\n",
      "TIMESTEP 274189 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.535679 / Loss  0.04883810877799988\n",
      "fps: 9.005851017003378\n",
      "TIMESTEP 274190 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.946269 / Loss  0.04337059333920479\n",
      "fps: 7.5778260767447705\n",
      "TIMESTEP 274191 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.600999 / Loss  0.0356995053589344\n",
      "fps: 6.892087654953925\n",
      "TIMESTEP 274192 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.282183 / Loss  0.03571530804038048\n",
      "fps: 9.27428352839463\n",
      "TIMESTEP 274193 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.8483515 / Loss  0.03427505865693092\n",
      "fps: 8.963126481191408\n",
      "TIMESTEP 274194 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.129612 / Loss  0.030317354947328568\n",
      "fps: 8.818121039596674\n",
      "TIMESTEP 274195 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.750341 / Loss  0.04330911487340927\n",
      "fps: 8.524436369105324\n",
      "TIMESTEP 274196 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.428212 / Loss  0.027424819767475128\n",
      "fps: 7.268918354508257\n",
      "TIMESTEP 274197 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.111296 / Loss  0.07905817031860352\n",
      "fps: 7.293097272502808\n",
      "TIMESTEP 274198 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.776497 / Loss  0.336351603269577\n",
      "fps: 7.588986815226812\n",
      "TIMESTEP 274199 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.262582 / Loss  0.02868451178073883\n",
      "fps: 7.070140010316194\n",
      "TIMESTEP 274200 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.182411 / Loss  0.05997202917933464\n",
      "fps: 7.725056543168038\n",
      "TIMESTEP 274201 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.396017 / Loss  0.11884089559316635\n",
      "fps: 7.086110102127876\n",
      "TIMESTEP 274202 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.608667 / Loss  0.019927658140659332\n",
      "fps: 8.114971172077546\n",
      "TIMESTEP 274203 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.874126 / Loss  0.0521252378821373\n",
      "fps: 7.651633288212523\n",
      "TIMESTEP 274204 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.975534 / Loss  0.04709688574075699\n",
      "fps: 7.943938119805753\n",
      "TIMESTEP 274205 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.831105 / Loss  0.02272706851363182\n",
      "fps: 7.808048371393414\n",
      "TIMESTEP 274206 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.062501 / Loss  0.03586786985397339\n",
      "fps: 8.821533582073856\n",
      "TIMESTEP 274207 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.997995 / Loss  0.046664465218782425\n",
      "fps: 7.595844863306434\n",
      "TIMESTEP 274208 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.010029 / Loss  4.097674369812012\n",
      "fps: 7.456619981724249\n",
      "TIMESTEP 274209 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.609015 / Loss  0.07631761580705643\n",
      "fps: 7.378167905360834\n",
      "TIMESTEP 274210 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.208365 / Loss  0.03306685760617256\n",
      "fps: 7.758867294878483\n",
      "TIMESTEP 274211 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  12.233327 / Loss  0.04616444185376167\n",
      "fps: 8.854327941042978\n",
      "TIMESTEP 274212 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.43081 / Loss  0.38759076595306396\n",
      "fps: 9.019019419375509\n",
      "TIMESTEP 274213 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.966938 / Loss  0.04600849747657776\n",
      "fps: 9.003975716358827\n",
      "TIMESTEP 274214 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.907212 / Loss  0.04222940281033516\n",
      "fps: 8.92467769013569\n",
      "TIMESTEP 274215 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.811292 / Loss  0.07197009027004242\n",
      "fps: 7.332673661980203\n",
      "TIMESTEP 274216 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.907876 / Loss  0.13809333741664886\n",
      "fps: 7.535959408665905\n",
      "TIMESTEP 274217 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.815991 / Loss  0.049339018762111664\n",
      "fps: 7.756987533127556\n",
      "TIMESTEP 274218 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.008408 / Loss  0.013982350006699562\n",
      "fps: 7.209755326134414\n",
      "TIMESTEP 274219 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.351735 / Loss  0.05725773423910141\n",
      "fps: 5.647928233150694\n",
      "TIMESTEP 274220 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.764031 / Loss  0.05797075480222702\n",
      "fps: 4.5413297097722465\n",
      "TIMESTEP 274221 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.346444 / Loss  0.05447947978973389\n",
      "fps: 6.616289341510815\n",
      "TIMESTEP 274222 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.915836 / Loss  0.02877379022538662\n",
      "fps: 7.374354087511802\n",
      "TIMESTEP 274223 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.01423 / Loss  0.06768849492073059\n",
      "fps: 9.082649585420308\n",
      "TIMESTEP 274224 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.644305 / Loss  0.03571295738220215\n",
      "fps: 9.309421499343017\n",
      "TIMESTEP 274225 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.582756 / Loss  0.03451147302985191\n",
      "fps: 9.204615580436892\n",
      "TIMESTEP 274226 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.231677 / Loss  0.08027994632720947\n",
      "fps: 5.1226388593733585\n",
      "TIMESTEP 274227 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.008584 / Loss  0.02863418310880661\n",
      "fps: 4.804576502810485\n",
      "TIMESTEP 274228 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.611629 / Loss  0.03245425969362259\n",
      "fps: 7.367838221447744\n",
      "TIMESTEP 274229 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  11.3994 / Loss  0.019853772595524788\n",
      "fps: 6.918553708102402\n",
      "TIMESTEP 274230 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.552928 / Loss  0.035538509488105774\n",
      "fps: 7.130999236626945\n",
      "TIMESTEP 274231 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 1 / REWARD 0.1 / Q_MAX  11.7849045 / Loss  0.021065857261419296\n",
      "fps: 8.821292391818707\n",
      "TIMESTEP 274232 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  10.935732 / Loss  0.045332226902246475\n",
      "fps: 8.787802726661317\n",
      "TIMESTEP 274233 / STATE train / EPSILON 9.999999987391849e-05 / ACTION 0 / REWARD 0.1 / Q_MAX  12.205021 / Loss  0.9068239331245422\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4829a0a02061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-12216cee503d>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(observe)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d8111a223a7c>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(model, game_state, observe)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m#run the selected action and observed next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mx_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fps: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# helpful for measuring frame rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-35ecfd7f7a27>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrab_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#display the image on screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_crashed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mscores_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;31m# log the score when game is over\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c4f41eeba315>\u001b[0m in \u001b[0;36mshow_img\u001b[0;34m(graphs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mimS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
