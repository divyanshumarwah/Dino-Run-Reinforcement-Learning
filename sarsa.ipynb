{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"chrome://dino\"\n",
    "chrome_driver_path = \"./chrome_driver/chromedriver\"\n",
    "loss_file_path = \"./Object2/loss_df.csv\"\n",
    "actions_file_path = \"./Object2/actions_df.csv\"\n",
    "q_value_file_path = \"./Object2/q_values.csv\"\n",
    "scores_file_path = \"./Object2/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.get('chrome://dino')\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('Object2'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('Object2' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[:300, :500] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                #new_q=np.argmax(q)\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1 # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                #print(Q_sa.shape)\n",
    "                new_q=np.argmax(Q_sa)\n",
    "                max_Q1 = (1-epsilon)*(np.mean(Q_sa))+epsilon*np.max(Q_sa)\n",
    "                #print(max_Q1)\n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * max_Q1\n",
    "                    #targets[i, action_t] = (targets[i, action_t]* (1-LEARNING_RATE)) + (LEARNING_RATE * (reward_t + GAMMA * max_Q1))\n",
    "                    \n",
    "                                                                                                         \n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./Object2/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./Object2/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./Object2/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)    \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 35000 / STATE explore / EPSILON 0.06513589899994673 / ACTION 1 / REWARD 0.1 / Q_MAX  2.0405805 / Loss  0.03135138377547264\n",
      "fps: 0.34784630819088824\n",
      "TIMESTEP 35001 / STATE explore / EPSILON 0.06513489999994673 / ACTION 1 / REWARD 0.1 / Q_MAX  1.8002865 / Loss  0.07434339076280594\n",
      "fps: 8.84963878198636\n",
      "TIMESTEP 35002 / STATE explore / EPSILON 0.06513390099994673 / ACTION 0 / REWARD 0.1 / Q_MAX  1.8489022 / Loss  0.026890242472290993\n",
      "fps: 8.934791824214216\n",
      "TIMESTEP 35003 / STATE explore / EPSILON 0.06513290199994673 / ACTION 0 / REWARD 0.1 / Q_MAX  1.7485832 / Loss  0.022807065397500992\n",
      "fps: 8.347089576446754\n",
      "TIMESTEP 35004 / STATE explore / EPSILON 0.06513190299994673 / ACTION 0 / REWARD 0.1 / Q_MAX  1.7725152 / Loss  0.022467520087957382\n",
      "fps: 8.565240387266204\n",
      "TIMESTEP 35005 / STATE explore / EPSILON 0.06513090399994673 / ACTION 0 / REWARD 0.1 / Q_MAX  2.2414863 / Loss  0.07213863730430603\n",
      "fps: 7.410876271719503\n",
      "TIMESTEP 35006 / STATE explore / EPSILON 0.06512990499994672 / ACTION 1 / REWARD 0.1 / Q_MAX  1.8767047 / Loss  0.0817856639623642\n",
      "fps: 8.391662565173723\n",
      "TIMESTEP 35007 / STATE explore / EPSILON 0.06512890599994672 / ACTION 0 / REWARD 0.1 / Q_MAX  1.7124921 / Loss  0.029404066503047943\n",
      "fps: 8.090676913343055\n",
      "TIMESTEP 35008 / STATE explore / EPSILON 0.06512790699994672 / ACTION 0 / REWARD 0.1 / Q_MAX  1.3740395 / Loss  0.0586366280913353\n",
      "fps: 7.20472722275663\n",
      "TIMESTEP 35009 / STATE explore / EPSILON 0.06512690799994672 / ACTION 1 / REWARD 0.1 / Q_MAX  1.7029157 / Loss  0.04525159299373627\n",
      "fps: 8.479679760629157\n",
      "TIMESTEP 35010 / STATE explore / EPSILON 0.06512590899994672 / ACTION 0 / REWARD 0.1 / Q_MAX  1.9493301 / Loss  0.11464501917362213\n",
      "fps: 7.30246810407265\n",
      "TIMESTEP 35011 / STATE explore / EPSILON 0.06512490999994672 / ACTION 1 / REWARD 0.1 / Q_MAX  1.6783065 / Loss  0.05327627435326576\n",
      "fps: 8.600824749878502\n",
      "TIMESTEP 35012 / STATE explore / EPSILON 0.06512391099994672 / ACTION 0 / REWARD 0.1 / Q_MAX  1.8351383 / Loss  0.05942237377166748\n",
      "fps: 7.863425535345354\n",
      "TIMESTEP 35013 / STATE explore / EPSILON 0.06512291199994671 / ACTION 0 / REWARD 0.1 / Q_MAX  1.8190176 / Loss  0.053257569670677185\n",
      "fps: 7.405707667018032\n",
      "TIMESTEP 35014 / STATE explore / EPSILON 0.06512191299994671 / ACTION 1 / REWARD 0.1 / Q_MAX  1.7588152 / Loss  0.10825654864311218\n",
      "fps: 8.545747199504488\n",
      "TIMESTEP 35015 / STATE explore / EPSILON 0.06512091399994671 / ACTION 0 / REWARD 0.1 / Q_MAX  1.6922286 / Loss  0.09447167813777924\n",
      "fps: 8.36698770377345\n",
      "TIMESTEP 35016 / STATE explore / EPSILON 0.06511991499994671 / ACTION 0 / REWARD 0.1 / Q_MAX  1.8586023 / Loss  0.10656492412090302\n",
      "----------Random Action----------\n",
      "fps: 7.27484394214542\n",
      "TIMESTEP 35017 / STATE explore / EPSILON 0.06511891599994671 / ACTION 1 / REWARD 0.1 / Q_MAX  0.8366053 / Loss  0.0381648987531662\n",
      "----------Random Action----------\n",
      "fps: 7.432004918872274\n",
      "TIMESTEP 35018 / STATE explore / EPSILON 0.0651179169999467 / ACTION 1 / REWARD 0.1 / Q_MAX  1.582619 / Loss  0.05587401241064072\n",
      "fps: 7.660843216151993\n",
      "TIMESTEP 35019 / STATE explore / EPSILON 0.0651169179999467 / ACTION 1 / REWARD 0.1 / Q_MAX  1.6358978 / Loss  0.09706835448741913\n",
      "fps: 8.802206058265794\n",
      "TIMESTEP 35020 / STATE explore / EPSILON 0.0651159189999467 / ACTION 0 / REWARD 0.1 / Q_MAX  1.578715 / Loss  0.05010579153895378\n",
      "fps: 8.382036483397016\n",
      "TIMESTEP 35021 / STATE explore / EPSILON 0.0651149199999467 / ACTION 0 / REWARD 0.1 / Q_MAX  0.8436943 / Loss  0.09086565673351288\n",
      "fps: 7.820073049178617\n",
      "TIMESTEP 35022 / STATE explore / EPSILON 0.0651139209999467 / ACTION 0 / REWARD 0.1 / Q_MAX  1.5673785 / Loss  0.0310121551156044\n",
      "fps: 8.335278885689359\n",
      "TIMESTEP 35023 / STATE explore / EPSILON 0.0651129219999467 / ACTION 0 / REWARD 0.1 / Q_MAX  1.9297723 / Loss  0.045522451400756836\n",
      "fps: 8.523622175256056\n",
      "TIMESTEP 35024 / STATE explore / EPSILON 0.0651119229999467 / ACTION 0 / REWARD 0.1 / Q_MAX  0.7347987 / Loss  0.047891728579998016\n",
      "fps: 8.559245110523394\n",
      "TIMESTEP 35025 / STATE explore / EPSILON 0.0651109239999467 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4537406 / Loss  0.05822066217660904\n",
      "fps: 7.028495685848679\n",
      "TIMESTEP 35026 / STATE explore / EPSILON 0.0651099249999467 / ACTION 1 / REWARD 0.1 / Q_MAX  1.3446331 / Loss  0.029130719602108\n",
      "fps: 9.126762279653844\n",
      "TIMESTEP 35027 / STATE explore / EPSILON 0.06510892599994669 / ACTION 0 / REWARD 0.1 / Q_MAX  1.7259955 / Loss  0.009807305410504341\n",
      "fps: 6.978701030756304\n",
      "TIMESTEP 35028 / STATE explore / EPSILON 0.06510792699994669 / ACTION 1 / REWARD 0.1 / Q_MAX  1.7666028 / Loss  0.04677949100732803\n",
      "fps: 7.339872357776218\n",
      "TIMESTEP 35029 / STATE explore / EPSILON 0.06510692799994669 / ACTION 1 / REWARD 0.1 / Q_MAX  1.7617894 / Loss  0.022135108709335327\n",
      "fps: 8.85511306657321\n",
      "TIMESTEP 35030 / STATE explore / EPSILON 0.06510592899994669 / ACTION 0 / REWARD 0.1 / Q_MAX  1.746918 / Loss  0.036147139966487885\n",
      "fps: 8.59270185444682\n",
      "TIMESTEP 35031 / STATE explore / EPSILON 0.06510492999994669 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4149857 / Loss  0.13473966717720032\n",
      "fps: 8.66409145180148\n",
      "TIMESTEP 35032 / STATE explore / EPSILON 0.06510393099994669 / ACTION 0 / REWARD 0.1 / Q_MAX  1.5294231 / Loss  0.02630755491554737\n",
      "fps: 9.387304557235131\n",
      "TIMESTEP 35033 / STATE explore / EPSILON 0.06510293199994668 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4868978 / Loss  0.05096950754523277\n",
      "fps: 7.486312678152939\n",
      "TIMESTEP 35034 / STATE explore / EPSILON 0.06510193299994668 / ACTION 1 / REWARD 0.1 / Q_MAX  1.5627582 / Loss  0.18305648863315582\n",
      "fps: 7.717650868770804\n",
      "TIMESTEP 35035 / STATE explore / EPSILON 0.06510093399994668 / ACTION 1 / REWARD 0.1 / Q_MAX  1.0477018 / Loss  0.03195595741271973\n",
      "fps: 7.504865104727687\n",
      "TIMESTEP 35036 / STATE explore / EPSILON 0.06509993499994668 / ACTION 1 / REWARD 0.1 / Q_MAX  1.2088373 / Loss  0.05972135066986084\n",
      "fps: 7.644966790367384\n",
      "TIMESTEP 35037 / STATE explore / EPSILON 0.06509893599994668 / ACTION 1 / REWARD 0.1 / Q_MAX  1.6677915 / Loss  0.04785311594605446\n",
      "fps: 8.582135147577882\n",
      "TIMESTEP 35038 / STATE explore / EPSILON 0.06509793699994668 / ACTION 0 / REWARD 0.1 / Q_MAX  0.2687667 / Loss  0.06777338683605194\n",
      "fps: 7.5305337960727\n",
      "TIMESTEP 35039 / STATE explore / EPSILON 0.06509693799994667 / ACTION 1 / REWARD 0.1 / Q_MAX  1.2347436 / Loss  0.07799618691205978\n",
      "fps: 7.68138306152536\n",
      "TIMESTEP 35040 / STATE explore / EPSILON 0.06509593899994667 / ACTION 1 / REWARD 0.1 / Q_MAX  1.2500511 / Loss  0.08447933197021484\n",
      "fps: 8.64615980837178\n",
      "TIMESTEP 35041 / STATE explore / EPSILON 0.06509493999994667 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4345819 / Loss  0.0662025511264801\n",
      "fps: 7.260839835128215\n",
      "TIMESTEP 35042 / STATE explore / EPSILON 0.06509394099994667 / ACTION 1 / REWARD 0.1 / Q_MAX  1.4546463 / Loss  0.03899911791086197\n",
      "fps: 7.25024675672639\n",
      "TIMESTEP 35043 / STATE explore / EPSILON 0.06509294199994667 / ACTION 1 / REWARD 0.1 / Q_MAX  1.615861 / Loss  0.03272348642349243\n",
      "fps: 8.424394829234588\n",
      "TIMESTEP 35044 / STATE explore / EPSILON 0.06509194299994667 / ACTION 0 / REWARD -1 / Q_MAX  1.2888606 / Loss  0.17169125378131866\n",
      "fps: 8.717574387070492\n",
      "TIMESTEP 35045 / STATE explore / EPSILON 0.06509094399994667 / ACTION 0 / REWARD 0.1 / Q_MAX  1.228869 / Loss  0.21692019701004028\n",
      "fps: 7.616438256662993\n",
      "TIMESTEP 35046 / STATE explore / EPSILON 0.06508994499994666 / ACTION 1 / REWARD 0.1 / Q_MAX  1.6144998 / Loss  0.09976295381784439\n",
      "fps: 8.827047446676417\n",
      "TIMESTEP 35047 / STATE explore / EPSILON 0.06508894599994666 / ACTION 0 / REWARD 0.1 / Q_MAX  1.3476717 / Loss  0.012405721470713615\n",
      "fps: 7.19698825984152\n",
      "TIMESTEP 35048 / STATE explore / EPSILON 0.06508794699994666 / ACTION 1 / REWARD 0.1 / Q_MAX  1.0950061 / Loss  0.023880794644355774\n",
      "fps: 9.062475557504953\n",
      "TIMESTEP 35049 / STATE explore / EPSILON 0.06508694799994666 / ACTION 0 / REWARD 0.1 / Q_MAX  0.59790856 / Loss  0.027406014502048492\n",
      "fps: 8.888817305022856\n",
      "TIMESTEP 35050 / STATE explore / EPSILON 0.06508594899994666 / ACTION 0 / REWARD 0.1 / Q_MAX  1.2584246 / Loss  0.022548355162143707\n",
      "fps: 9.40910018821323\n",
      "TIMESTEP 35051 / STATE explore / EPSILON 0.06508494999994666 / ACTION 0 / REWARD 0.1 / Q_MAX  0.5104622 / Loss  0.0539257638156414\n",
      "fps: 9.551916849234582\n",
      "TIMESTEP 35052 / STATE explore / EPSILON 0.06508395099994665 / ACTION 0 / REWARD 0.1 / Q_MAX  0.34386897 / Loss  0.027579329907894135\n",
      "fps: 9.439572211769578\n",
      "TIMESTEP 35053 / STATE explore / EPSILON 0.06508295199994665 / ACTION 0 / REWARD 0.1 / Q_MAX  1.39666 / Loss  0.06358716636896133\n",
      "fps: 7.8556493270552625\n",
      "TIMESTEP 35054 / STATE explore / EPSILON 0.06508195299994665 / ACTION 1 / REWARD 0.1 / Q_MAX  0.6491046 / Loss  0.04790005832910538\n",
      "fps: 8.899718853772134\n",
      "TIMESTEP 35055 / STATE explore / EPSILON 0.06508095399994665 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.055617757 / Loss  0.06676774471998215\n",
      "----------Random Action----------\n",
      "fps: 7.389398725535622\n",
      "TIMESTEP 35056 / STATE explore / EPSILON 0.06507995499994665 / ACTION 1 / REWARD 0.1 / Q_MAX  1.2289182 / Loss  0.060458991676568985\n",
      "fps: 7.407407930685533\n",
      "TIMESTEP 35057 / STATE explore / EPSILON 0.06507895599994665 / ACTION 1 / REWARD 0.1 / Q_MAX  1.5554702 / Loss  0.056819457560777664\n",
      "fps: 7.2573221141950475\n",
      "TIMESTEP 35058 / STATE explore / EPSILON 0.06507795699994665 / ACTION 1 / REWARD 0.1 / Q_MAX  1.272905 / Loss  0.03321078047156334\n",
      "fps: 8.547819689537322\n",
      "TIMESTEP 35059 / STATE explore / EPSILON 0.06507695799994664 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.34364548 / Loss  0.043137695640325546\n",
      "fps: 8.884938430610779\n",
      "TIMESTEP 35060 / STATE explore / EPSILON 0.06507595899994664 / ACTION 0 / REWARD 0.1 / Q_MAX  1.2715698 / Loss  0.05949119105935097\n",
      "fps: 9.03090395078794\n",
      "TIMESTEP 35061 / STATE explore / EPSILON 0.06507495999994664 / ACTION 0 / REWARD 0.1 / Q_MAX  1.3192383 / Loss  0.012057827785611153\n",
      "fps: 8.59818004030233\n",
      "TIMESTEP 35062 / STATE explore / EPSILON 0.06507396099994664 / ACTION 0 / REWARD 0.1 / Q_MAX  1.423455 / Loss  0.04050542414188385\n",
      "fps: 8.678828008607846\n",
      "TIMESTEP 35063 / STATE explore / EPSILON 0.06507296199994664 / ACTION 0 / REWARD 0.1 / Q_MAX  0.21250519 / Loss  0.04520738869905472\n",
      "fps: 8.355902995674928\n",
      "TIMESTEP 35064 / STATE explore / EPSILON 0.06507196299994664 / ACTION 0 / REWARD 0.1 / Q_MAX  1.2365413 / Loss  0.08058632165193558\n",
      "fps: 8.676799208925242\n",
      "TIMESTEP 35065 / STATE explore / EPSILON 0.06507096399994663 / ACTION 0 / REWARD 0.1 / Q_MAX  1.6258276 / Loss  0.01866227388381958\n",
      "fps: 8.726153052371847\n",
      "TIMESTEP 35066 / STATE explore / EPSILON 0.06506996499994663 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4724033 / Loss  0.03431274741888046\n",
      "fps: 8.644538197886632\n",
      "TIMESTEP 35067 / STATE explore / EPSILON 0.06506896599994663 / ACTION 0 / REWARD 0.1 / Q_MAX  1.6815618 / Loss  0.05921144410967827\n",
      "fps: 6.53287852592558\n",
      "TIMESTEP 35068 / STATE explore / EPSILON 0.06506796699994663 / ACTION 1 / REWARD 0.1 / Q_MAX  1.4114746 / Loss  0.0162509772926569\n",
      "fps: 6.736041473277808\n",
      "TIMESTEP 35069 / STATE explore / EPSILON 0.06506696799994663 / ACTION 1 / REWARD 0.1 / Q_MAX  1.6982024 / Loss  0.03351251408457756\n",
      "fps: 8.262001095216895\n",
      "TIMESTEP 35070 / STATE explore / EPSILON 0.06506596899994663 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4152454 / Loss  0.02098165825009346\n",
      "fps: 6.704803801990834\n",
      "TIMESTEP 35071 / STATE explore / EPSILON 0.06506496999994663 / ACTION 1 / REWARD 0.1 / Q_MAX  1.5479698 / Loss  0.06059170886874199\n",
      "fps: 7.831213544739613\n",
      "TIMESTEP 35072 / STATE explore / EPSILON 0.06506397099994662 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.17518406 / Loss  0.10752107203006744\n",
      "fps: 6.384189545239375\n",
      "TIMESTEP 35073 / STATE explore / EPSILON 0.06506297199994662 / ACTION 1 / REWARD 0.1 / Q_MAX  1.2530372 / Loss  0.008837796747684479\n",
      "fps: 6.921944803389097\n",
      "TIMESTEP 35074 / STATE explore / EPSILON 0.06506197299994662 / ACTION 1 / REWARD 0.1 / Q_MAX  1.4508163 / Loss  0.02427014894783497\n",
      "fps: 8.50311089126173\n",
      "TIMESTEP 35075 / STATE explore / EPSILON 0.06506097399994662 / ACTION 0 / REWARD 0.1 / Q_MAX  1.44008 / Loss  0.028566213324666023\n",
      "fps: 6.6292354330185965\n",
      "TIMESTEP 35076 / STATE explore / EPSILON 0.06505997499994662 / ACTION 1 / REWARD 0.1 / Q_MAX  1.5955986 / Loss  0.050148364156484604\n",
      "fps: 6.653010530790547\n",
      "TIMESTEP 35077 / STATE explore / EPSILON 0.06505897599994662 / ACTION 1 / REWARD 0.1 / Q_MAX  1.6761323 / Loss  0.018483860418200493\n",
      "fps: 7.006036714717624\n",
      "TIMESTEP 35078 / STATE explore / EPSILON 0.06505797699994662 / ACTION 1 / REWARD 0.1 / Q_MAX  0.52263993 / Loss  0.029026217758655548\n",
      "fps: 8.199206333691722\n",
      "TIMESTEP 35079 / STATE explore / EPSILON 0.06505697799994661 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4045295 / Loss  0.030997250229120255\n",
      "fps: 7.98887657185087\n",
      "TIMESTEP 35080 / STATE explore / EPSILON 0.06505597899994661 / ACTION 0 / REWARD 0.1 / Q_MAX  1.6629385 / Loss  0.11261150240898132\n",
      "fps: 8.468995707253248\n",
      "TIMESTEP 35081 / STATE explore / EPSILON 0.06505497999994661 / ACTION 0 / REWARD 0.1 / Q_MAX  1.798765 / Loss  0.0488174632191658\n",
      "fps: 6.385647493350669\n",
      "TIMESTEP 35082 / STATE explore / EPSILON 0.06505398099994661 / ACTION 1 / REWARD -1 / Q_MAX  1.6936944 / Loss  0.024385472759604454\n",
      "fps: 6.339014455955809\n",
      "TIMESTEP 35083 / STATE explore / EPSILON 0.06505298199994661 / ACTION 1 / REWARD 0.1 / Q_MAX  1.528821 / Loss  0.055140502750873566\n",
      "fps: 7.1389735192894905\n",
      "TIMESTEP 35084 / STATE explore / EPSILON 0.0650519829999466 / ACTION 1 / REWARD 0.1 / Q_MAX  1.7000031 / Loss  0.021633559837937355\n",
      "fps: 8.890626092965384\n",
      "TIMESTEP 35085 / STATE explore / EPSILON 0.0650509839999466 / ACTION 0 / REWARD 0.1 / Q_MAX  0.89017206 / Loss  0.024444522336125374\n",
      "fps: 7.321448333766816\n",
      "TIMESTEP 35086 / STATE explore / EPSILON 0.0650499849999466 / ACTION 1 / REWARD 0.1 / Q_MAX  1.9594373 / Loss  0.0652569979429245\n",
      "fps: 7.513617220234779\n",
      "TIMESTEP 35087 / STATE explore / EPSILON 0.0650489859999466 / ACTION 1 / REWARD 0.1 / Q_MAX  1.5025054 / Loss  0.02797173708677292\n",
      "fps: 8.586931776305553\n",
      "TIMESTEP 35088 / STATE explore / EPSILON 0.0650479869999466 / ACTION 0 / REWARD 0.1 / Q_MAX  1.9792923 / Loss  0.1032995954155922\n",
      "fps: 4.728800297642536\n",
      "TIMESTEP 35089 / STATE explore / EPSILON 0.0650469879999466 / ACTION 0 / REWARD 0.1 / Q_MAX  1.4310677 / Loss  0.24411320686340332\n",
      "fps: 4.011653419796658\n",
      "TIMESTEP 35090 / STATE explore / EPSILON 0.0650459889999466 / ACTION 1 / REWARD 0.1 / Q_MAX  1.6138971 / Loss  0.05240898206830025\n",
      "fps: 6.195985162621743\n",
      "TIMESTEP 35091 / STATE explore / EPSILON 0.0650449899999466 / ACTION 0 / REWARD 0.1 / Q_MAX  1.9341407 / Loss  0.0626869797706604\n",
      "fps: 7.881955876273161\n",
      "TIMESTEP 35092 / STATE explore / EPSILON 0.0650439909999466 / ACTION 0 / REWARD 0.1 / Q_MAX  2.2610974 / Loss  0.09052130579948425\n",
      "fps: 8.569790510983207\n",
      "TIMESTEP 35093 / STATE explore / EPSILON 0.06504299199994659 / ACTION 0 / REWARD 0.1 / Q_MAX  1.7431048 / Loss  0.02754625491797924\n",
      "fps: 8.511514266001056\n",
      "TIMESTEP 35094 / STATE explore / EPSILON 0.06504199299994659 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.18806392 / Loss  0.04350186139345169\n",
      "fps: 8.46045417686998\n",
      "TIMESTEP 35095 / STATE explore / EPSILON 0.06504099399994659 / ACTION 0 / REWARD 0.1 / Q_MAX  1.2933239 / Loss  0.04459279403090477\n",
      "fps: 6.528272343392744\n",
      "TIMESTEP 35096 / STATE explore / EPSILON 0.06503999499994659 / ACTION 0 / REWARD 0.1 / Q_MAX  1.5044779 / Loss  0.04437458515167236\n",
      "fps: 6.698111761248946\n",
      "TIMESTEP 35097 / STATE explore / EPSILON 0.06503899599994659 / ACTION 0 / REWARD 0.1 / Q_MAX  0.14850819 / Loss  0.08026367425918579\n",
      "fps: 6.741270655096652\n",
      "TIMESTEP 35098 / STATE explore / EPSILON 0.06503799699994658 / ACTION 1 / REWARD 0.1 / Q_MAX  1.8360814 / Loss  0.08506455272436142\n",
      "fps: 8.198613334584342\n",
      "TIMESTEP 35099 / STATE explore / EPSILON 0.06503699799994658 / ACTION 0 / REWARD 0.1 / Q_MAX  1.9058176 / Loss  0.02308795414865017\n",
      "fps: 7.89186260047566\n",
      "TIMESTEP 35100 / STATE explore / EPSILON 0.06503599899994658 / ACTION 0 / REWARD 0.1 / Q_MAX  2.0293405 / Loss  0.023933513090014458\n",
      "fps: 8.388641554566219\n",
      "TIMESTEP 35101 / STATE explore / EPSILON 0.06503499999994658 / ACTION 0 / REWARD 0.1 / Q_MAX  1.7012925 / Loss  0.03939652815461159\n",
      "----------Random Action----------\n",
      "fps: 8.915514932511424\n",
      "TIMESTEP 35102 / STATE explore / EPSILON 0.06503400099994658 / ACTION 0 / REWARD 0.1 / Q_MAX  1.7041525 / Loss  0.02912389487028122\n",
      "fps: 6.810958450318845\n",
      "TIMESTEP 35103 / STATE explore / EPSILON 0.06503300199994658 / ACTION 1 / REWARD 0.1 / Q_MAX  1.9237938 / Loss  0.023445865139365196\n",
      "fps: 8.297435775696645\n"
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
